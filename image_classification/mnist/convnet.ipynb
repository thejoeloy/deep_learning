{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8f08bb3-69d7-459d-88a9-64534eb5644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to tensors\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),  # Normalize to match MNIST stats\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop((227, 227)),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_ds = torchvision.datasets.MNIST(\"/home/eagle/Projects/dl_from_scratch/mnist\", train=True, download=True, transform=transform)\n",
    "test_ds = torchvision.datasets.MNIST(\"/home/eagle/Projects/dl_from_scratch/mnist\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(train_ds))  # 80% for training\n",
    "val_size = len(train_ds) - train_size  # 20% for validation\n",
    "\n",
    "# Split the train_dataset into train and val\n",
    "train_ds, val_ds = random_split(train_ds, [train_size, val_size])\n",
    "\n",
    "batch_size = 80\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=6)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=6)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52af8ecd-197b-45c7-8039-8140e599d35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DCNN, self).__init__()\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        self.conv1 = nn.Conv2d(3, 96, kernel_size=11, stride=4)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(96)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv5 = nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.fc6 = nn.Linear(256 * 6 * 6, 4096)  # Adjusted based on (227x227 input)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.d6 = nn.Dropout()\n",
    "\n",
    "        self.fc7 = nn.Linear(4096, 4096)\n",
    "        self.relu7 = nn.ReLU()\n",
    "        self.d7 = nn.Dropout()\n",
    "\n",
    "        self.fc8 = nn.Linear(4096, 10)\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.bn1(self.pool1(self.relu1(self.conv1(x))))\n",
    "        x = self.bn2(self.pool2(self.relu2(self.conv2(x))))\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.relu4(self.conv4(x))\n",
    "        x = self.pool5(self.relu5(self.conv5(x)))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.d6(self.relu6(self.fc6(x)))\n",
    "        x = self.d7(self.relu7(self.fc7(x)))\n",
    "        x = self.fc8(x)\n",
    "        x = self.dequant(x)\n",
    "        return x  \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = DCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a5a4e20-c08d-486e-92ac-8c09b314305b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_301698/3215076074.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "Training Epoch 1:   0%|                              | 0/600 [00:00<?, ?batch/s]/tmp/ipykernel_301698/3215076074.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Training Epoch 1: 100%|████████████████████| 600/600 [00:30<00:00, 19.42batch/s]\n",
      "Validating:   0%|                                    | 0/150 [00:00<?, ?batch/s]/tmp/ipykernel_301698/3215076074.py:50: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Enable autocasting for validation\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:06<00:00, 22.84batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Training Loss: 1.0348, Validation Loss: 0.1741, Validation Accuracy: 94.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|████████████████████| 600/600 [00:30<00:00, 19.68batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:06<00:00, 23.73batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/30], Training Loss: 0.1531, Validation Loss: 0.0912, Validation Accuracy: 97.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|████████████████████| 600/600 [00:32<00:00, 18.72batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:06<00:00, 22.94batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/30], Training Loss: 0.0990, Validation Loss: 0.0670, Validation Accuracy: 97.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|████████████████████| 600/600 [00:31<00:00, 19.18batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:07<00:00, 21.23batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/30], Training Loss: 0.0768, Validation Loss: 0.0609, Validation Accuracy: 98.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|████████████████████| 600/600 [00:33<00:00, 17.99batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:06<00:00, 24.50batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30], Training Loss: 0.0663, Validation Loss: 0.0504, Validation Accuracy: 98.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|████████████████████| 600/600 [00:29<00:00, 20.07batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:05<00:00, 26.63batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/30], Training Loss: 0.0570, Validation Loss: 0.0580, Validation Accuracy: 98.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|████████████████████| 600/600 [00:29<00:00, 20.09batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:06<00:00, 23.73batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/30], Training Loss: 0.0503, Validation Loss: 0.0465, Validation Accuracy: 98.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|████████████████████| 600/600 [00:30<00:00, 19.64batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:05<00:00, 26.71batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/30], Training Loss: 0.0470, Validation Loss: 0.0400, Validation Accuracy: 98.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|████████████████████| 600/600 [00:29<00:00, 20.28batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:05<00:00, 26.33batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/30], Training Loss: 0.0432, Validation Loss: 0.0369, Validation Accuracy: 98.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|███████████████████| 600/600 [00:29<00:00, 20.40batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:07<00:00, 19.85batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30], Training Loss: 0.0388, Validation Loss: 0.0338, Validation Accuracy: 98.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|███████████████████| 600/600 [00:30<00:00, 19.60batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:06<00:00, 22.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/30], Training Loss: 0.0371, Validation Loss: 0.0340, Validation Accuracy: 98.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|███████████████████| 600/600 [00:29<00:00, 20.37batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:06<00:00, 23.19batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/30], Training Loss: 0.0346, Validation Loss: 0.0287, Validation Accuracy: 99.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|███████████████████| 600/600 [00:28<00:00, 20.73batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:05<00:00, 26.71batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/30], Training Loss: 0.0328, Validation Loss: 0.0302, Validation Accuracy: 99.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|███████████████████| 600/600 [00:29<00:00, 20.49batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:05<00:00, 28.68batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/30], Training Loss: 0.0311, Validation Loss: 0.0260, Validation Accuracy: 99.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|███████████████████| 600/600 [00:29<00:00, 20.31batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:06<00:00, 23.90batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/30], Training Loss: 0.0288, Validation Loss: 0.0275, Validation Accuracy: 99.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|███████████████████| 600/600 [00:30<00:00, 20.00batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:07<00:00, 20.50batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/30], Training Loss: 0.0266, Validation Loss: 0.0296, Validation Accuracy: 99.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|███████████████████| 600/600 [00:32<00:00, 18.73batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:07<00:00, 21.26batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/30], Training Loss: 0.0274, Validation Loss: 0.0320, Validation Accuracy: 99.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18: 100%|███████████████████| 600/600 [00:32<00:00, 18.60batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:07<00:00, 19.60batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/30], Training Loss: 0.0262, Validation Loss: 0.0283, Validation Accuracy: 99.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19: 100%|███████████████████| 600/600 [00:31<00:00, 18.78batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:07<00:00, 20.90batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/30], Training Loss: 0.0237, Validation Loss: 0.0263, Validation Accuracy: 99.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20: 100%|███████████████████| 600/600 [00:32<00:00, 18.59batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:06<00:00, 21.91batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/30], Training Loss: 0.0233, Validation Loss: 0.0276, Validation Accuracy: 99.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 21: 100%|███████████████████| 600/600 [00:31<00:00, 18.76batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:07<00:00, 19.88batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/30], Training Loss: 0.0186, Validation Loss: 0.0217, Validation Accuracy: 99.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 22: 100%|███████████████████| 600/600 [00:33<00:00, 18.02batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:06<00:00, 22.07batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/30], Training Loss: 0.0175, Validation Loss: 0.0215, Validation Accuracy: 99.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 23: 100%|███████████████████| 600/600 [00:33<00:00, 17.88batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:08<00:00, 18.74batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/30], Training Loss: 0.0159, Validation Loss: 0.0206, Validation Accuracy: 99.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 24: 100%|███████████████████| 600/600 [00:33<00:00, 17.76batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:07<00:00, 21.40batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/30], Training Loss: 0.0160, Validation Loss: 0.0201, Validation Accuracy: 99.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25: 100%|███████████████████| 600/600 [00:32<00:00, 18.51batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:07<00:00, 20.34batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/30], Training Loss: 0.0168, Validation Loss: 0.0200, Validation Accuracy: 99.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 26: 100%|███████████████████| 600/600 [00:32<00:00, 18.68batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:07<00:00, 20.37batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/30], Training Loss: 0.0159, Validation Loss: 0.0205, Validation Accuracy: 99.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 27: 100%|███████████████████| 600/600 [00:31<00:00, 18.76batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:07<00:00, 21.36batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/30], Training Loss: 0.0149, Validation Loss: 0.0215, Validation Accuracy: 99.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 28: 100%|███████████████████| 600/600 [00:34<00:00, 17.52batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:07<00:00, 20.79batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/30], Training Loss: 0.0149, Validation Loss: 0.0193, Validation Accuracy: 99.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 29: 100%|███████████████████| 600/600 [00:31<00:00, 18.90batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:07<00:00, 20.84batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/30], Training Loss: 0.0147, Validation Loss: 0.0203, Validation Accuracy: 99.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 30: 100%|███████████████████| 600/600 [00:31<00:00, 19.03batch/s]\n",
      "Validating: 100%|██████████████████████████| 150/150 [00:06<00:00, 21.64batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/30], Training Loss: 0.0144, Validation Loss: 0.0212, Validation Accuracy: 99.42%\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Import tqdm for progress bars\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "initial_lr = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr, momentum=0.9, weight_decay=0.0005)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Adjust learning rate if epoch is greater than or equal to 20\n",
    "    if epoch >= 20:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.0001  # Set learning rate to 0.0001\n",
    "\n",
    "    # Wrap the train_loader with tqdm for progress tracking\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\", unit=\"batch\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass with mixed precision\n",
    "        with torch.cuda.amp.autocast():  \n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()  # Scale the loss for stable gradients\n",
    "        scaler.step(optimizer)  # Update the parameters\n",
    "        scaler.update()  # Update the scaler\n",
    "\n",
    "        running_loss += loss.item()   \n",
    "    \n",
    "    # Calculate average loss for the epoch\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Wrap the val_loader with tqdm for progress tracking\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validating\", unit=\"batch\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():  # Enable autocasting for validation\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                \n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Calculate average validation loss and accuracy\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "    accuracy = correct / total * 100  # Convert to percentage\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "          f\"Training Loss: {avg_loss:.4f}, \"\n",
    "          f\"Validation Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88361453-0bbe-4664-8783-4fcc99833d38",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_quantized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 85\u001b[0m\n\u001b[1;32m     80\u001b[0m model_fp32_prepared \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mquantization\u001b[38;5;241m.\u001b[39mprepare_qat(model_fp32_fused)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Step 6: Apply pruning\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Example: Prune 20% of weights from all conv layers\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m model_quantized\u001b[38;5;241m.\u001b[39mnamed_modules():\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, nn\u001b[38;5;241m.\u001b[39mConv2d):\n\u001b[1;32m     87\u001b[0m         prune\u001b[38;5;241m.\u001b[39mln_structured(module, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m, amount\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_quantized' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.quantization import QuantStub, DeQuantStub, fuse_modules\n",
    "import torch.nn.utils.prune as prune\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DCNN, self).__init__()\n",
    "        self.quant = QuantStub()\n",
    "        self.conv1 = nn.Conv2d(3, 96, kernel_size=11, stride=4)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(96)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv5 = nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.fc6 = nn.Linear(256 * 6 * 6, 4096)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.d6 = nn.Dropout()\n",
    "\n",
    "        self.fc7 = nn.Linear(4096, 4096)\n",
    "        self.relu7 = nn.ReLU()\n",
    "        self.d7 = nn.Dropout()\n",
    "\n",
    "        self.fc8 = nn.Linear(4096, 10)\n",
    "        self.dequant = DeQuantStub()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.bn1(self.pool1(self.relu1(self.conv1(x))))\n",
    "        x = self.bn2(self.pool2(self.relu2(self.conv2(x))))\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.relu4(self.conv4(x))\n",
    "        x = self.pool5(self.relu5(self.conv5(x)))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.d6(self.relu6(self.fc6(x)))\n",
    "        x = self.d7(self.relu7(self.fc7(x)))\n",
    "        x = self.fc8(x)\n",
    "        x = self.dequant(x)\n",
    "        return x  \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = DCNN().to(device)\n",
    "\n",
    "# Step 1: Define quantization configuration\n",
    "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# Step 2: Fuse Conv and ReLU layers\n",
    "model_fp32_fused = fuse_modules(model, [['conv1', 'relu1'],\n",
    "                                          ['conv2', 'relu2'],\n",
    "                                          ['conv3', 'relu3'],\n",
    "                                          ['conv4', 'relu4'],\n",
    "                                          ['conv5', 'relu5']])\n",
    "\n",
    "# Step 3: Prepare the model for quantization\n",
    "model_fp32_prepared = torch.quantization.prepare_qat(model_fp32_fused)\n",
    "\n",
    "# Step 4: Calibrate the model (run a few batches through it to collect statistics)\n",
    "model_fp32_prepared.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in tqdm(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        model_fp32_prepared(inputs)\n",
    "\n",
    "# Step 5: Convert to quantized model\n",
    "model_quantized = torch.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "# Step 6: Apply pruning\n",
    "# Example: Prune 20% of weights from all conv layers\n",
    "for name, module in model_quantized.named_modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        prune.ln_structured(module, name='weight', amount=0.2, n=2, dim=0)\n",
    "\n",
    "# Optionally, fine-tune the model after pruning\n",
    "# Define optimizer and loss function as before\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "initial_lr = 0.001\n",
    "optimizer = torch.optim.SGD(model_quantized.parameters(), lr=initial_lr, momentum=0.9, weight_decay=0.0005)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_quantized.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Adjust learning rate if epoch is greater than or equal to 20\n",
    "    if epoch >= 20:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.0001\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\", unit=\"batch\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model_quantized(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()   \n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model_quantized.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validating\", unit=\"batch\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model_quantized(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                \n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "    accuracy = correct / total * 100\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "          f\"Training Loss: {avg_loss:.4f}, \"\n",
    "          f\"Validation Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d90b17e-96ee-45d4-956e-3cb16ca5c41a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
