{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddd281c8-32b8-4b25-925c-bb90a6b9cf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-04 10:39:45.278515: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-04 10:39:45.295206: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-04 10:39:45.300462: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-04 10:39:45.313165: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-04 10:39:46.147347: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.21 (you have 1.4.7). Upgrade using: pip install --upgrade albumentations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image_id', 'image', 'width', 'height', 'objects'],\n",
      "        num_rows: 700\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image_id', 'image', 'width', 'height', 'objects'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image_id', 'image', 'width', 'height', 'objects'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n",
      "['animals', 'cat', 'chicken', 'cow', 'dog', 'fox', 'goat', 'horse', 'person', 'racoon', 'skunk']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoImageProcessor\n",
    "import albumentations as A\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"Francesco/animals-ij5d2\")\n",
    "train_ds = ds[\"train\"]\n",
    "val_ds = ds[\"validation\"]\n",
    "test_ds = ds[\"test\"]\n",
    "print(ds)\n",
    "categories = train_ds.features[\"objects\"].feature[\"category\"].names\n",
    "id2label = {index: x for index, x in enumerate(categories, start=0)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "print(categories)\n",
    "checkpoint = \"PekingU/rtdetr_r50vd_coco_o365\"\n",
    "image_size = 480\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    checkpoint,\n",
    "    do_resize=True,\n",
    "    size={\"width\": image_size, \"height\": image_size},\n",
    ")\n",
    "\n",
    "train_augmentation_and_transform = A.Compose(\n",
    "    [\n",
    "        A.Perspective(p=0.1),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.HueSaturationValue(p=0.1),\n",
    "        A.Rotate(limit=20, p=0.5),\n",
    "        A.RandomScale(scale_limit=0.2, p=0.5),\n",
    "        A.RandomResizedCrop(height=224, width=224, scale=(0.8, 1.0), ratio=(0.75, 1.33), p=0.5),\n",
    "        A.Affine(scale=(0.9, 1.1), translate_percent=0.1, rotate=(-10, 10), shear=(-5, 5), p=0.5),\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "        A.MotionBlur(blur_limit=3, p=0.2),\n",
    "        A.GridDistortion(p=0.2),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category\"], clip=True, min_area=25),\n",
    ")\n",
    "\n",
    "# to make sure boxes are clipped to image size and there is no boxes with area < 1 pixel\n",
    "validation_transform = A.Compose(\n",
    "    [A.NoOp()],\n",
    "    bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category\"], clip=True, min_area=1),\n",
    ")\n",
    "\n",
    "class AnimalsDataset(Dataset):\n",
    "    def __init__(self, dataset, image_processor, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.image_processor = image_processor\n",
    "        self.transform = transform\n",
    "\n",
    "    @staticmethod\n",
    "    def format_image_annotations_as_coco(image_id, categories, boxes):\n",
    "        \"\"\"Format one set of image annotations to the COCO format\n",
    "\n",
    "        Args:\n",
    "            image_id (str): image id. e.g. \"0001\"\n",
    "            categories (List[int]): list of categories/class labels corresponding to provided bounding boxes\n",
    "            boxes (List[Tuple[float]]): list of bounding boxes provided in COCO format\n",
    "                ([center_x, center_y, width, height] in absolute coordinates)\n",
    "\n",
    "        Returns:\n",
    "            dict: {\n",
    "                \"image_id\": image id,\n",
    "                \"annotations\": list of formatted annotations\n",
    "            }\n",
    "        \"\"\"\n",
    "        annotations = []\n",
    "        for category, bbox in zip(categories, boxes):\n",
    "            formatted_annotation = {\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category,\n",
    "                \"bbox\": list(bbox),\n",
    "                \"iscrowd\": 0,\n",
    "                \"area\": bbox[2] * bbox[3],\n",
    "            }\n",
    "            annotations.append(formatted_annotation)\n",
    "\n",
    "        return {\n",
    "            \"image_id\": image_id,\n",
    "            \"annotations\": annotations,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "\n",
    "        image_id = sample[\"image_id\"]\n",
    "        image = sample[\"image\"]\n",
    "        boxes = sample[\"objects\"][\"bbox\"]\n",
    "        categories = sample[\"objects\"][\"category\"]\n",
    "\n",
    "        # Convert image to RGB numpy array\n",
    "        image = np.array(image.convert(\"RGB\"))\n",
    "\n",
    "        # Apply augmentations\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, bboxes=boxes, category=categories)\n",
    "            image = transformed[\"image\"]\n",
    "            boxes = transformed[\"bboxes\"]\n",
    "            categories = transformed[\"category\"]\n",
    "\n",
    "        # Format annotations in COCO format for image_processor\n",
    "        formatted_annotations = self.format_image_annotations_as_coco(image_id, categories, boxes)\n",
    "\n",
    "        # Apply the image processor transformations: resizing, rescaling, normalization\n",
    "        result = self.image_processor(\n",
    "            images=image, annotations=formatted_annotations, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Image processor expands batch dimension, lets squeeze it\n",
    "        result = {k: v[0] for k, v in result.items()}\n",
    "\n",
    "        return result\n",
    "\n",
    "train_ds = AnimalsDataset(train_ds, image_processor, transform=train_augmentation_and_transform)\n",
    "val_ds = AnimalsDataset(val_ds, image_processor, transform=validation_transform)\n",
    "test_ds = AnimalsDataset(test_ds, image_processor, transform=validation_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1b8f36-2b98-4c4d-8c74-db9498d581a6",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# Get mapping from category id to category name\n",
    "\n",
    "\n",
    "print(categories)\n",
    "print(id2label)\n",
    "print(label2id)\n",
    "\n",
    "# Load image and annotations\n",
    "image = train_ds[2][\"image\"]\n",
    "annotations = train_ds[2][\"objects\"]\n",
    "\n",
    "# Draw bounding boxes and labels\n",
    "draw = ImageDraw.Draw(image)\n",
    "for i in range(len(annotations[\"id\"])):\n",
    "    box = annotations[\"bbox\"][i]\n",
    "    class_idx = annotations[\"category\"][i]\n",
    "    x, y, w, h = tuple(box)\n",
    "    draw.rectangle((x, y, x + w, y + h), outline=\"red\", width=1)\n",
    "    draw.text((x, y), id2label[class_idx], fill=\"white\")\n",
    "\n",
    "image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in [15, 16, 17]:\n",
    "    image = train_ds[i][\"image\"]\n",
    "    annotations = train_ds[i][\"objects\"]\n",
    "\n",
    "    # Apply the augmentation\n",
    "    output = train_augmentation_and_transform(image=np.array(image), bboxes=annotations[\"bbox\"], category=annotations[\"category\"])\n",
    "\n",
    "    # Unpack the output\n",
    "    image = Image.fromarray(output[\"image\"])\n",
    "    categories, boxes = output[\"category\"], output[\"bboxes\"]\n",
    "\n",
    "    # Draw the augmented image\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for category, box in zip(categories, boxes):\n",
    "        x, y, w, h = box\n",
    "        draw.rectangle((x, y, x + w, y + h), outline=\"red\", width=1)\n",
    "        draw.text((x, y), id2label[category], fill=\"white\")\n",
    "    image.show()\n",
    "\n",
    "for i in [15, 16, 17]:\n",
    "    sample = train_ds[i]\n",
    "\n",
    "    # De-normalize image\n",
    "    image = sample[\"pixel_values\"]\n",
    "    print(\"Image tensor shape:\", image.shape)\n",
    "    image = image.numpy().transpose(1, 2, 0)\n",
    "    image = (image - image.min()) / (image.max() - image.min()) * 255.\n",
    "    image = Image.fromarray(image.astype(np.uint8))\n",
    "\n",
    "    # Convert boxes from [center_x, center_y, width, height] to [x, y, width, height] for visualization\n",
    "    boxes = sample[\"labels\"][\"boxes\"].numpy()\n",
    "    print(\"Boxes shape:\", boxes.shape)\n",
    "    boxes[:, :2] = boxes[:, :2] - boxes[:, 2:] / 2\n",
    "    w, h = image.size\n",
    "    boxes = boxes * np.array([w, h, w, h])[None]\n",
    "\n",
    "    categories = sample[\"labels\"][\"class_labels\"].numpy()\n",
    "    print(\"Categories shape:\", categories.shape)\n",
    "\n",
    "    # Draw boxes and labels on image\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box, category in zip(boxes, categories):\n",
    "        x, y, w, h = box\n",
    "        draw.rectangle([x, y, x + w, y + h], outline=\"red\", width=1)\n",
    "        draw.text((x, y), id2label[category], fill=\"white\")\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e044909-e117-4de3-9b7f-a969b326bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.image_transforms import center_to_corners_format\n",
    "import torch\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "def collate_fn(batch):\n",
    "    data = {}\n",
    "    data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n",
    "    data[\"labels\"] = [x[\"labels\"] for x in batch]\n",
    "    return data\n",
    "\n",
    "def convert_bbox_yolo_to_pascal(boxes, image_size):\n",
    "    \"\"\"\n",
    "    Convert bounding boxes from YOLO format (x_center, y_center, width, height) in range [0, 1]\n",
    "    to Pascal VOC format (x_min, y_min, x_max, y_max) in absolute coordinates.\n",
    "\n",
    "    Args:\n",
    "        boxes (torch.Tensor): Bounding boxes in YOLO format\n",
    "        image_size (Tuple[int, int]): Image size in format (height, width)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Bounding boxes in Pascal VOC format (x_min, y_min, x_max, y_max)\n",
    "    \"\"\"\n",
    "    # convert center to corners format\n",
    "    boxes = center_to_corners_format(boxes)\n",
    "\n",
    "    # convert to absolute coordinates\n",
    "    height, width = image_size\n",
    "    boxes = boxes * torch.tensor([[width, height, width, height]])\n",
    "\n",
    "    return boxes\n",
    "\n",
    "@dataclass\n",
    "class ModelOutput:\n",
    "    logits: torch.Tensor\n",
    "    pred_boxes: torch.Tensor\n",
    "\n",
    "\n",
    "class MAPEvaluator:\n",
    "\n",
    "    def __init__(self, image_processor, threshold=0.00, id2label=None):\n",
    "        self.image_processor = image_processor\n",
    "        self.threshold = threshold\n",
    "        self.id2label = id2label\n",
    "\n",
    "    def collect_image_sizes(self, targets):\n",
    "        \"\"\"Collect image sizes across the dataset as list of tensors with shape [batch_size, 2].\"\"\"\n",
    "        image_sizes = []\n",
    "        for batch in targets:\n",
    "            batch_image_sizes = torch.tensor(np.array([x[\"size\"] for x in batch]))\n",
    "            image_sizes.append(batch_image_sizes)\n",
    "        return image_sizes\n",
    "\n",
    "    def collect_targets(self, targets, image_sizes):\n",
    "        post_processed_targets = []\n",
    "        for target_batch, image_size_batch in zip(targets, image_sizes):\n",
    "            for target, size in zip(target_batch, image_size_batch):\n",
    "                boxes = torch.tensor(target[\"boxes\"])\n",
    "                boxes = convert_bbox_yolo_to_pascal(boxes, size)\n",
    "                labels = torch.tensor(target[\"class_labels\"])\n",
    "                post_processed_targets.append({\"boxes\": boxes, \"labels\": labels})\n",
    "        return post_processed_targets\n",
    "\n",
    "    def collect_predictions(self, predictions, image_sizes):\n",
    "        post_processed_predictions = []\n",
    "        for batch, target_sizes in zip(predictions, image_sizes):\n",
    "            batch_logits, batch_boxes = batch[1], batch[2]\n",
    "            output = ModelOutput(logits=torch.tensor(batch_logits), pred_boxes=torch.tensor(batch_boxes))\n",
    "            post_processed_output = self.image_processor.post_process_object_detection(\n",
    "                output, threshold=self.threshold, target_sizes=target_sizes\n",
    "            )\n",
    "            post_processed_predictions.extend(post_processed_output)\n",
    "        return post_processed_predictions\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, evaluation_results):\n",
    "\n",
    "        predictions, targets = evaluation_results.predictions, evaluation_results.label_ids\n",
    "\n",
    "        image_sizes = self.collect_image_sizes(targets)\n",
    "        post_processed_targets = self.collect_targets(targets, image_sizes)\n",
    "        post_processed_predictions = self.collect_predictions(predictions, image_sizes)\n",
    "\n",
    "        evaluator = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n",
    "        evaluator.warn_on_many_detections = False\n",
    "        evaluator.update(post_processed_predictions, post_processed_targets)\n",
    "\n",
    "        metrics = evaluator.compute()\n",
    "\n",
    "        # Replace list of per class metrics with separate metric for each class\n",
    "        classes = metrics.pop(\"classes\")\n",
    "        map_per_class = metrics.pop(\"map_per_class\")\n",
    "        mar_100_per_class = metrics.pop(\"mar_100_per_class\")\n",
    "        for class_id, class_map, class_mar in zip(classes, map_per_class, mar_100_per_class):\n",
    "            class_name = id2label[class_id.item()] if id2label is not None else class_id.item()\n",
    "            metrics[f\"map_{class_name}\"] = class_map\n",
    "            metrics[f\"mar_100_{class_name}\"] = class_mar\n",
    "\n",
    "        metrics = {k: round(v.item(), 4) for k, v in metrics.items()}\n",
    "\n",
    "        return metrics\n",
    "\n",
    "eval_compute_metrics_fn = MAPEvaluator(image_processor=image_processor, threshold=0.01, id2label=id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "832e75ad-e119-491b-a9d6-6d71b370a38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not load the custom kernel for multi-scale deformable attention: Error building extension 'MultiScaleDeformableAttention': [1/2] /usr/local/cuda-11.5/bin/nvcc --generate-dependencies-with-compile --dependency-output ms_deform_attn_cuda.cuda.o.d -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/eagle/anaconda3/lib/python3.11/site-packages/transformers/kernels/deformable_detr -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include/TH -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda-11.5/include -isystem /home/eagle/anaconda3/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -std=c++17 -c /home/eagle/anaconda3/lib/python3.11/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu -o ms_deform_attn_cuda.cuda.o \n",
      "\u001b[31mFAILED: \u001b[0mms_deform_attn_cuda.cuda.o \n",
      "/usr/local/cuda-11.5/bin/nvcc --generate-dependencies-with-compile --dependency-output ms_deform_attn_cuda.cuda.o.d -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/eagle/anaconda3/lib/python3.11/site-packages/transformers/kernels/deformable_detr -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include/TH -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda-11.5/include -isystem /home/eagle/anaconda3/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -std=c++17 -c /home/eagle/anaconda3/lib/python3.11/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu -o ms_deform_attn_cuda.cuda.o \n",
      "/bin/sh: 1: /usr/local/cuda-11.5/bin/nvcc: not found\n",
      "ninja: build stopped: subcommand failed.\n",
      "\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/eagle/.cache/torch_extensions/py311_cu121/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/eagle/.cache/torch_extensions/py311_cu121/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/eagle/.cache/torch_extensions/py311_cu121/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/eagle/.cache/torch_extensions/py311_cu121/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/eagle/.cache/torch_extensions/py311_cu121/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Some weights of RTDetrForObjectDetection were not initialized from the model checkpoint at PekingU/rtdetr_r50vd_coco_o365 and are newly initialized because the shapes did not match:\n",
      "- model.decoder.class_embed.0.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([11]) in the model instantiated\n",
      "- model.decoder.class_embed.0.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([11, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.1.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([11]) in the model instantiated\n",
      "- model.decoder.class_embed.1.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([11, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.2.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([11]) in the model instantiated\n",
      "- model.decoder.class_embed.2.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([11, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.3.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([11]) in the model instantiated\n",
      "- model.decoder.class_embed.3.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([11, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.4.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([11]) in the model instantiated\n",
      "- model.decoder.class_embed.4.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([11, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.5.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([11]) in the model instantiated\n",
      "- model.decoder.class_embed.5.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([11, 256]) in the model instantiated\n",
      "- model.denoising_class_embed.weight: found shape torch.Size([81, 256]) in the checkpoint and torch.Size([12, 256]) in the model instantiated\n",
      "- model.enc_score_head.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([11]) in the model instantiated\n",
      "- model.enc_score_head.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([11, 256]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/eagle/anaconda3/lib/python3.11/site-packages/accelerate/accelerator.py:469: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4400' max='4400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4400/4400 1:12:07, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Map</th>\n",
       "      <th>Map 50</th>\n",
       "      <th>Map 75</th>\n",
       "      <th>Map Small</th>\n",
       "      <th>Map Medium</th>\n",
       "      <th>Map Large</th>\n",
       "      <th>Mar 1</th>\n",
       "      <th>Mar 10</th>\n",
       "      <th>Mar 100</th>\n",
       "      <th>Mar Small</th>\n",
       "      <th>Mar Medium</th>\n",
       "      <th>Mar Large</th>\n",
       "      <th>Map Animals</th>\n",
       "      <th>Mar 100 Animals</th>\n",
       "      <th>Map Cat</th>\n",
       "      <th>Mar 100 Cat</th>\n",
       "      <th>Map Chicken</th>\n",
       "      <th>Mar 100 Chicken</th>\n",
       "      <th>Map Cow</th>\n",
       "      <th>Mar 100 Cow</th>\n",
       "      <th>Map Dog</th>\n",
       "      <th>Mar 100 Dog</th>\n",
       "      <th>Map Fox</th>\n",
       "      <th>Mar 100 Fox</th>\n",
       "      <th>Map Goat</th>\n",
       "      <th>Mar 100 Goat</th>\n",
       "      <th>Map Horse</th>\n",
       "      <th>Mar 100 Horse</th>\n",
       "      <th>Map Person</th>\n",
       "      <th>Mar 100 Person</th>\n",
       "      <th>Map Racoon</th>\n",
       "      <th>Mar 100 Racoon</th>\n",
       "      <th>Map Skunk</th>\n",
       "      <th>Mar 100 Skunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>174.083694</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.029100</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>0.136500</td>\n",
       "      <td>0.213300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.214600</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>0.614300</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.186700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>0.185600</td>\n",
       "      <td>0.441200</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.488900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>146.844086</td>\n",
       "      <td>0.027400</td>\n",
       "      <td>0.031800</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.126800</td>\n",
       "      <td>0.209700</td>\n",
       "      <td>0.239200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.239900</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.041500</td>\n",
       "      <td>0.657100</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>0.186100</td>\n",
       "      <td>0.558800</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.566700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>109.644470</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.055500</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063700</td>\n",
       "      <td>0.144700</td>\n",
       "      <td>0.249400</td>\n",
       "      <td>0.283600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285800</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.121900</td>\n",
       "      <td>0.557100</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.090500</td>\n",
       "      <td>0.288100</td>\n",
       "      <td>0.594100</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.711100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.116000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.346700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>88.634216</td>\n",
       "      <td>0.088700</td>\n",
       "      <td>0.101800</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102900</td>\n",
       "      <td>0.206700</td>\n",
       "      <td>0.340500</td>\n",
       "      <td>0.382100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384300</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.508100</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.413300</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.095200</td>\n",
       "      <td>0.286500</td>\n",
       "      <td>0.605900</td>\n",
       "      <td>0.050100</td>\n",
       "      <td>0.688900</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.118200</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>0.493300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>68.970810</td>\n",
       "      <td>0.108400</td>\n",
       "      <td>0.123400</td>\n",
       "      <td>0.115500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124900</td>\n",
       "      <td>0.348700</td>\n",
       "      <td>0.559500</td>\n",
       "      <td>0.611500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.623100</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.340500</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.490500</td>\n",
       "      <td>0.470400</td>\n",
       "      <td>0.852900</td>\n",
       "      <td>0.052800</td>\n",
       "      <td>0.822200</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.587500</td>\n",
       "      <td>0.083700</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.118200</td>\n",
       "      <td>0.102500</td>\n",
       "      <td>0.693300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>53.694229</td>\n",
       "      <td>0.169700</td>\n",
       "      <td>0.195400</td>\n",
       "      <td>0.183100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.486400</td>\n",
       "      <td>0.703200</td>\n",
       "      <td>0.769200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.783600</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.277500</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.027700</td>\n",
       "      <td>0.926700</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.738100</td>\n",
       "      <td>0.522500</td>\n",
       "      <td>0.935300</td>\n",
       "      <td>0.249700</td>\n",
       "      <td>0.877800</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.593100</td>\n",
       "      <td>0.046000</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.344900</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.327300</td>\n",
       "      <td>0.177300</td>\n",
       "      <td>0.886700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>40.944603</td>\n",
       "      <td>0.239800</td>\n",
       "      <td>0.282900</td>\n",
       "      <td>0.259700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249200</td>\n",
       "      <td>0.551500</td>\n",
       "      <td>0.747900</td>\n",
       "      <td>0.795700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.812400</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.385900</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.873300</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>0.709500</td>\n",
       "      <td>0.588200</td>\n",
       "      <td>0.911800</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>0.866700</td>\n",
       "      <td>0.123200</td>\n",
       "      <td>0.762100</td>\n",
       "      <td>0.083200</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.640400</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.372700</td>\n",
       "      <td>0.199400</td>\n",
       "      <td>0.953300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>32.074699</td>\n",
       "      <td>0.316600</td>\n",
       "      <td>0.378800</td>\n",
       "      <td>0.348900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.337300</td>\n",
       "      <td>0.559700</td>\n",
       "      <td>0.786500</td>\n",
       "      <td>0.834800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.851900</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.553900</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.089300</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.197700</td>\n",
       "      <td>0.728600</td>\n",
       "      <td>0.654600</td>\n",
       "      <td>0.935300</td>\n",
       "      <td>0.414700</td>\n",
       "      <td>0.933300</td>\n",
       "      <td>0.132900</td>\n",
       "      <td>0.737900</td>\n",
       "      <td>0.139200</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.720300</td>\n",
       "      <td>0.844000</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.253400</td>\n",
       "      <td>0.966700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>26.869062</td>\n",
       "      <td>0.409000</td>\n",
       "      <td>0.483900</td>\n",
       "      <td>0.443600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.434000</td>\n",
       "      <td>0.605100</td>\n",
       "      <td>0.835700</td>\n",
       "      <td>0.868900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.886400</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.610200</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.364300</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.388300</td>\n",
       "      <td>0.742900</td>\n",
       "      <td>0.674600</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.502700</td>\n",
       "      <td>0.922200</td>\n",
       "      <td>0.196700</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>0.145200</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.715800</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>0.836400</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>0.953300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>24.118023</td>\n",
       "      <td>0.477700</td>\n",
       "      <td>0.582900</td>\n",
       "      <td>0.527000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.500100</td>\n",
       "      <td>0.595700</td>\n",
       "      <td>0.815200</td>\n",
       "      <td>0.854600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.870700</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.302300</td>\n",
       "      <td>0.886700</td>\n",
       "      <td>0.423100</td>\n",
       "      <td>0.733300</td>\n",
       "      <td>0.688900</td>\n",
       "      <td>0.929400</td>\n",
       "      <td>0.600700</td>\n",
       "      <td>0.866700</td>\n",
       "      <td>0.206300</td>\n",
       "      <td>0.855200</td>\n",
       "      <td>0.174000</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.718700</td>\n",
       "      <td>0.828000</td>\n",
       "      <td>0.150500</td>\n",
       "      <td>0.809100</td>\n",
       "      <td>0.703300</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>21.952244</td>\n",
       "      <td>0.528500</td>\n",
       "      <td>0.642300</td>\n",
       "      <td>0.594900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.545400</td>\n",
       "      <td>0.615300</td>\n",
       "      <td>0.823800</td>\n",
       "      <td>0.869600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.885500</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.705300</td>\n",
       "      <td>0.885700</td>\n",
       "      <td>0.408000</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.419600</td>\n",
       "      <td>0.723800</td>\n",
       "      <td>0.766900</td>\n",
       "      <td>0.905900</td>\n",
       "      <td>0.753900</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.450400</td>\n",
       "      <td>0.855200</td>\n",
       "      <td>0.236600</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.706600</td>\n",
       "      <td>0.848000</td>\n",
       "      <td>0.126900</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.710900</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>77.409100</td>\n",
       "      <td>19.843437</td>\n",
       "      <td>0.530400</td>\n",
       "      <td>0.646700</td>\n",
       "      <td>0.590400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.547400</td>\n",
       "      <td>0.608200</td>\n",
       "      <td>0.816500</td>\n",
       "      <td>0.863200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.878700</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.749700</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.321700</td>\n",
       "      <td>0.846700</td>\n",
       "      <td>0.472100</td>\n",
       "      <td>0.733300</td>\n",
       "      <td>0.779800</td>\n",
       "      <td>0.876500</td>\n",
       "      <td>0.678700</td>\n",
       "      <td>0.888900</td>\n",
       "      <td>0.462600</td>\n",
       "      <td>0.858600</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.732300</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>0.098100</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.926700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>77.409100</td>\n",
       "      <td>19.212547</td>\n",
       "      <td>0.551300</td>\n",
       "      <td>0.687800</td>\n",
       "      <td>0.624800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067300</td>\n",
       "      <td>0.563100</td>\n",
       "      <td>0.592400</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.851300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.867100</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.643600</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.440800</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.419900</td>\n",
       "      <td>0.723800</td>\n",
       "      <td>0.752800</td>\n",
       "      <td>0.917600</td>\n",
       "      <td>0.785100</td>\n",
       "      <td>0.911100</td>\n",
       "      <td>0.481200</td>\n",
       "      <td>0.844800</td>\n",
       "      <td>0.237600</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.689400</td>\n",
       "      <td>0.796000</td>\n",
       "      <td>0.241200</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.821600</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>77.409100</td>\n",
       "      <td>18.032394</td>\n",
       "      <td>0.595600</td>\n",
       "      <td>0.727600</td>\n",
       "      <td>0.671700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.612700</td>\n",
       "      <td>0.634500</td>\n",
       "      <td>0.819400</td>\n",
       "      <td>0.854300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.870300</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.803200</td>\n",
       "      <td>0.885700</td>\n",
       "      <td>0.535600</td>\n",
       "      <td>0.833300</td>\n",
       "      <td>0.511500</td>\n",
       "      <td>0.728600</td>\n",
       "      <td>0.767100</td>\n",
       "      <td>0.941200</td>\n",
       "      <td>0.799700</td>\n",
       "      <td>0.855600</td>\n",
       "      <td>0.531700</td>\n",
       "      <td>0.872400</td>\n",
       "      <td>0.246200</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.836400</td>\n",
       "      <td>0.764700</td>\n",
       "      <td>0.926700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>77.409100</td>\n",
       "      <td>17.122324</td>\n",
       "      <td>0.586100</td>\n",
       "      <td>0.706500</td>\n",
       "      <td>0.661200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.603500</td>\n",
       "      <td>0.613400</td>\n",
       "      <td>0.797400</td>\n",
       "      <td>0.863800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.879900</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.833300</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>0.846700</td>\n",
       "      <td>0.419900</td>\n",
       "      <td>0.733300</td>\n",
       "      <td>0.812300</td>\n",
       "      <td>0.911800</td>\n",
       "      <td>0.806000</td>\n",
       "      <td>0.911100</td>\n",
       "      <td>0.524000</td>\n",
       "      <td>0.855200</td>\n",
       "      <td>0.167800</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.746900</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.361200</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.804200</td>\n",
       "      <td>0.953300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>77.409100</td>\n",
       "      <td>16.410986</td>\n",
       "      <td>0.603000</td>\n",
       "      <td>0.744000</td>\n",
       "      <td>0.690900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067300</td>\n",
       "      <td>0.618600</td>\n",
       "      <td>0.619000</td>\n",
       "      <td>0.828800</td>\n",
       "      <td>0.857300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.873300</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.804300</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.491400</td>\n",
       "      <td>0.813300</td>\n",
       "      <td>0.542300</td>\n",
       "      <td>0.733300</td>\n",
       "      <td>0.808200</td>\n",
       "      <td>0.894100</td>\n",
       "      <td>0.791500</td>\n",
       "      <td>0.911100</td>\n",
       "      <td>0.478000</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>0.218600</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.777400</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.322300</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.795700</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>77.409100</td>\n",
       "      <td>15.936544</td>\n",
       "      <td>0.616800</td>\n",
       "      <td>0.757300</td>\n",
       "      <td>0.691800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067300</td>\n",
       "      <td>0.634200</td>\n",
       "      <td>0.626500</td>\n",
       "      <td>0.824100</td>\n",
       "      <td>0.862200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.878300</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.675100</td>\n",
       "      <td>0.885700</td>\n",
       "      <td>0.548300</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.468000</td>\n",
       "      <td>0.733300</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.823700</td>\n",
       "      <td>0.911100</td>\n",
       "      <td>0.539700</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>0.295800</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.722400</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>0.473800</td>\n",
       "      <td>0.890900</td>\n",
       "      <td>0.822200</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>77.409100</td>\n",
       "      <td>15.048120</td>\n",
       "      <td>0.664500</td>\n",
       "      <td>0.796100</td>\n",
       "      <td>0.749300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067300</td>\n",
       "      <td>0.679700</td>\n",
       "      <td>0.649300</td>\n",
       "      <td>0.845400</td>\n",
       "      <td>0.876200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.892200</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.808400</td>\n",
       "      <td>0.885700</td>\n",
       "      <td>0.501400</td>\n",
       "      <td>0.806700</td>\n",
       "      <td>0.568300</td>\n",
       "      <td>0.728600</td>\n",
       "      <td>0.826100</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.820100</td>\n",
       "      <td>0.911100</td>\n",
       "      <td>0.584100</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>0.361500</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.788600</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>0.549500</td>\n",
       "      <td>0.927300</td>\n",
       "      <td>0.837200</td>\n",
       "      <td>0.926700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>77.409100</td>\n",
       "      <td>14.727221</td>\n",
       "      <td>0.650500</td>\n",
       "      <td>0.785600</td>\n",
       "      <td>0.733700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067300</td>\n",
       "      <td>0.667900</td>\n",
       "      <td>0.636000</td>\n",
       "      <td>0.836800</td>\n",
       "      <td>0.876700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.892700</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.821100</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.485500</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.519500</td>\n",
       "      <td>0.728600</td>\n",
       "      <td>0.808200</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.933300</td>\n",
       "      <td>0.584900</td>\n",
       "      <td>0.851700</td>\n",
       "      <td>0.329800</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.802300</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.460900</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.834100</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>77.409100</td>\n",
       "      <td>14.174809</td>\n",
       "      <td>0.672800</td>\n",
       "      <td>0.803500</td>\n",
       "      <td>0.747600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074100</td>\n",
       "      <td>0.688900</td>\n",
       "      <td>0.658500</td>\n",
       "      <td>0.846800</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.891700</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.830300</td>\n",
       "      <td>0.871400</td>\n",
       "      <td>0.507800</td>\n",
       "      <td>0.853300</td>\n",
       "      <td>0.501200</td>\n",
       "      <td>0.742900</td>\n",
       "      <td>0.819100</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.836800</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.593600</td>\n",
       "      <td>0.858600</td>\n",
       "      <td>0.428100</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.781300</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.927300</td>\n",
       "      <td>0.854500</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>77.409100</td>\n",
       "      <td>13.899197</td>\n",
       "      <td>0.657300</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>0.737100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067300</td>\n",
       "      <td>0.674900</td>\n",
       "      <td>0.644000</td>\n",
       "      <td>0.847200</td>\n",
       "      <td>0.886100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.902000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.827100</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.486300</td>\n",
       "      <td>0.853300</td>\n",
       "      <td>0.481200</td>\n",
       "      <td>0.728600</td>\n",
       "      <td>0.823300</td>\n",
       "      <td>0.941200</td>\n",
       "      <td>0.877600</td>\n",
       "      <td>0.922200</td>\n",
       "      <td>0.593700</td>\n",
       "      <td>0.872400</td>\n",
       "      <td>0.300900</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.590400</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.794300</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>77.409100</td>\n",
       "      <td>13.789886</td>\n",
       "      <td>0.654800</td>\n",
       "      <td>0.797500</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101000</td>\n",
       "      <td>0.671500</td>\n",
       "      <td>0.635100</td>\n",
       "      <td>0.842500</td>\n",
       "      <td>0.882300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.897800</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.804100</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.486100</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.564100</td>\n",
       "      <td>0.757100</td>\n",
       "      <td>0.754700</td>\n",
       "      <td>0.870600</td>\n",
       "      <td>0.807200</td>\n",
       "      <td>0.933300</td>\n",
       "      <td>0.582200</td>\n",
       "      <td>0.851700</td>\n",
       "      <td>0.365800</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.813700</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.583600</td>\n",
       "      <td>0.918200</td>\n",
       "      <td>0.786500</td>\n",
       "      <td>0.913300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>22.551500</td>\n",
       "      <td>13.358076</td>\n",
       "      <td>0.671900</td>\n",
       "      <td>0.810100</td>\n",
       "      <td>0.738900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067300</td>\n",
       "      <td>0.686100</td>\n",
       "      <td>0.664700</td>\n",
       "      <td>0.845400</td>\n",
       "      <td>0.875500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.891400</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.724500</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.524700</td>\n",
       "      <td>0.866700</td>\n",
       "      <td>0.501100</td>\n",
       "      <td>0.728600</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>0.947100</td>\n",
       "      <td>0.879200</td>\n",
       "      <td>0.922200</td>\n",
       "      <td>0.596700</td>\n",
       "      <td>0.803400</td>\n",
       "      <td>0.330200</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.733500</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.708000</td>\n",
       "      <td>0.918200</td>\n",
       "      <td>0.877800</td>\n",
       "      <td>0.926700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>22.551500</td>\n",
       "      <td>12.991175</td>\n",
       "      <td>0.718300</td>\n",
       "      <td>0.853600</td>\n",
       "      <td>0.787700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.731400</td>\n",
       "      <td>0.649700</td>\n",
       "      <td>0.840900</td>\n",
       "      <td>0.887900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.903800</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.861400</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.642700</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.505000</td>\n",
       "      <td>0.728600</td>\n",
       "      <td>0.781600</td>\n",
       "      <td>0.894100</td>\n",
       "      <td>0.903200</td>\n",
       "      <td>0.944400</td>\n",
       "      <td>0.631700</td>\n",
       "      <td>0.851700</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.748600</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.756900</td>\n",
       "      <td>0.936400</td>\n",
       "      <td>0.894200</td>\n",
       "      <td>0.973300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>22.551500</td>\n",
       "      <td>12.833196</td>\n",
       "      <td>0.678900</td>\n",
       "      <td>0.806300</td>\n",
       "      <td>0.741500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.693600</td>\n",
       "      <td>0.665800</td>\n",
       "      <td>0.849700</td>\n",
       "      <td>0.887700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.903300</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.880500</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.540500</td>\n",
       "      <td>0.906700</td>\n",
       "      <td>0.474400</td>\n",
       "      <td>0.714300</td>\n",
       "      <td>0.818300</td>\n",
       "      <td>0.947100</td>\n",
       "      <td>0.866900</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>0.841400</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.752200</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>0.724700</td>\n",
       "      <td>0.936400</td>\n",
       "      <td>0.845600</td>\n",
       "      <td>0.913300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>22.551500</td>\n",
       "      <td>12.867283</td>\n",
       "      <td>0.670900</td>\n",
       "      <td>0.815700</td>\n",
       "      <td>0.748600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>0.687200</td>\n",
       "      <td>0.644700</td>\n",
       "      <td>0.849100</td>\n",
       "      <td>0.876800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.892800</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.569300</td>\n",
       "      <td>0.826700</td>\n",
       "      <td>0.555000</td>\n",
       "      <td>0.752400</td>\n",
       "      <td>0.786400</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.834600</td>\n",
       "      <td>0.922200</td>\n",
       "      <td>0.564000</td>\n",
       "      <td>0.831000</td>\n",
       "      <td>0.383800</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.814400</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.683800</td>\n",
       "      <td>0.890900</td>\n",
       "      <td>0.769700</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>22.551500</td>\n",
       "      <td>12.232146</td>\n",
       "      <td>0.705100</td>\n",
       "      <td>0.842300</td>\n",
       "      <td>0.779800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040700</td>\n",
       "      <td>0.722500</td>\n",
       "      <td>0.649800</td>\n",
       "      <td>0.849200</td>\n",
       "      <td>0.887800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.902900</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.883700</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.614000</td>\n",
       "      <td>0.873300</td>\n",
       "      <td>0.543000</td>\n",
       "      <td>0.742900</td>\n",
       "      <td>0.817900</td>\n",
       "      <td>0.911800</td>\n",
       "      <td>0.789200</td>\n",
       "      <td>0.922200</td>\n",
       "      <td>0.644700</td>\n",
       "      <td>0.875900</td>\n",
       "      <td>0.459600</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.791500</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.682500</td>\n",
       "      <td>0.872700</td>\n",
       "      <td>0.824800</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>22.551500</td>\n",
       "      <td>12.069789</td>\n",
       "      <td>0.680900</td>\n",
       "      <td>0.806400</td>\n",
       "      <td>0.760200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072100</td>\n",
       "      <td>0.697900</td>\n",
       "      <td>0.659900</td>\n",
       "      <td>0.852600</td>\n",
       "      <td>0.889700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.905200</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.859100</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.489700</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.421700</td>\n",
       "      <td>0.733300</td>\n",
       "      <td>0.814900</td>\n",
       "      <td>0.917600</td>\n",
       "      <td>0.880400</td>\n",
       "      <td>0.944400</td>\n",
       "      <td>0.599300</td>\n",
       "      <td>0.875900</td>\n",
       "      <td>0.399000</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.816300</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.690400</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.838700</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>22.551500</td>\n",
       "      <td>11.613651</td>\n",
       "      <td>0.748400</td>\n",
       "      <td>0.863600</td>\n",
       "      <td>0.821700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077800</td>\n",
       "      <td>0.764400</td>\n",
       "      <td>0.671700</td>\n",
       "      <td>0.876200</td>\n",
       "      <td>0.908200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.923300</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.888600</td>\n",
       "      <td>0.942900</td>\n",
       "      <td>0.586000</td>\n",
       "      <td>0.886700</td>\n",
       "      <td>0.523300</td>\n",
       "      <td>0.742900</td>\n",
       "      <td>0.791600</td>\n",
       "      <td>0.941200</td>\n",
       "      <td>0.909200</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.615100</td>\n",
       "      <td>0.865500</td>\n",
       "      <td>0.677800</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.850500</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.743300</td>\n",
       "      <td>0.927300</td>\n",
       "      <td>0.898500</td>\n",
       "      <td>0.966700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>22.551500</td>\n",
       "      <td>11.342094</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.873900</td>\n",
       "      <td>0.813700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>0.766600</td>\n",
       "      <td>0.676600</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.904600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.919500</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.893400</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.698900</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.569800</td>\n",
       "      <td>0.757100</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.947100</td>\n",
       "      <td>0.904800</td>\n",
       "      <td>0.966700</td>\n",
       "      <td>0.661500</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>0.537200</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.785200</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.727800</td>\n",
       "      <td>0.918200</td>\n",
       "      <td>0.911700</td>\n",
       "      <td>0.966700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>22.551500</td>\n",
       "      <td>11.634388</td>\n",
       "      <td>0.702400</td>\n",
       "      <td>0.840900</td>\n",
       "      <td>0.767700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072900</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.653600</td>\n",
       "      <td>0.867300</td>\n",
       "      <td>0.895000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.910500</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.889500</td>\n",
       "      <td>0.957100</td>\n",
       "      <td>0.596900</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.536900</td>\n",
       "      <td>0.733300</td>\n",
       "      <td>0.793600</td>\n",
       "      <td>0.935300</td>\n",
       "      <td>0.903300</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.617000</td>\n",
       "      <td>0.841400</td>\n",
       "      <td>0.433700</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.751800</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>0.683100</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.817800</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>22.551500</td>\n",
       "      <td>11.066170</td>\n",
       "      <td>0.741200</td>\n",
       "      <td>0.876700</td>\n",
       "      <td>0.805400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.756800</td>\n",
       "      <td>0.669100</td>\n",
       "      <td>0.861700</td>\n",
       "      <td>0.895100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.911300</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.884200</td>\n",
       "      <td>0.971400</td>\n",
       "      <td>0.644700</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.558800</td>\n",
       "      <td>0.738100</td>\n",
       "      <td>0.840300</td>\n",
       "      <td>0.941200</td>\n",
       "      <td>0.914800</td>\n",
       "      <td>0.933300</td>\n",
       "      <td>0.659100</td>\n",
       "      <td>0.844800</td>\n",
       "      <td>0.544500</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.735800</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.755700</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.874300</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>22.551500</td>\n",
       "      <td>10.806270</td>\n",
       "      <td>0.743100</td>\n",
       "      <td>0.866300</td>\n",
       "      <td>0.828600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.760800</td>\n",
       "      <td>0.666300</td>\n",
       "      <td>0.866800</td>\n",
       "      <td>0.892700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.908900</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.883700</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.520300</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.588400</td>\n",
       "      <td>0.738100</td>\n",
       "      <td>0.778700</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.909600</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.664700</td>\n",
       "      <td>0.848300</td>\n",
       "      <td>0.652200</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.814500</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.736200</td>\n",
       "      <td>0.890900</td>\n",
       "      <td>0.883000</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>22.551500</td>\n",
       "      <td>10.988729</td>\n",
       "      <td>0.735100</td>\n",
       "      <td>0.870800</td>\n",
       "      <td>0.806800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.751700</td>\n",
       "      <td>0.663500</td>\n",
       "      <td>0.858500</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.906000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.888500</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.567500</td>\n",
       "      <td>0.853300</td>\n",
       "      <td>0.537800</td>\n",
       "      <td>0.728600</td>\n",
       "      <td>0.783900</td>\n",
       "      <td>0.935300</td>\n",
       "      <td>0.904200</td>\n",
       "      <td>0.922200</td>\n",
       "      <td>0.630800</td>\n",
       "      <td>0.855200</td>\n",
       "      <td>0.626000</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.806700</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.735200</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.870200</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>17.474400</td>\n",
       "      <td>10.597839</td>\n",
       "      <td>0.764100</td>\n",
       "      <td>0.889100</td>\n",
       "      <td>0.856800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>0.777800</td>\n",
       "      <td>0.661000</td>\n",
       "      <td>0.865600</td>\n",
       "      <td>0.892300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.906500</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.890700</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.649100</td>\n",
       "      <td>0.886700</td>\n",
       "      <td>0.557600</td>\n",
       "      <td>0.752400</td>\n",
       "      <td>0.825500</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.901500</td>\n",
       "      <td>0.944400</td>\n",
       "      <td>0.679800</td>\n",
       "      <td>0.848300</td>\n",
       "      <td>0.656800</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.768400</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.868800</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>17.474400</td>\n",
       "      <td>10.813790</td>\n",
       "      <td>0.750700</td>\n",
       "      <td>0.875500</td>\n",
       "      <td>0.834500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.765600</td>\n",
       "      <td>0.662100</td>\n",
       "      <td>0.850900</td>\n",
       "      <td>0.892400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.907700</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.872200</td>\n",
       "      <td>0.942900</td>\n",
       "      <td>0.626100</td>\n",
       "      <td>0.886700</td>\n",
       "      <td>0.485200</td>\n",
       "      <td>0.723800</td>\n",
       "      <td>0.798500</td>\n",
       "      <td>0.929400</td>\n",
       "      <td>0.933300</td>\n",
       "      <td>0.966700</td>\n",
       "      <td>0.694300</td>\n",
       "      <td>0.865500</td>\n",
       "      <td>0.693300</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.815200</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.706600</td>\n",
       "      <td>0.890900</td>\n",
       "      <td>0.882900</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>17.474400</td>\n",
       "      <td>10.548456</td>\n",
       "      <td>0.754900</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.839900</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.038500</td>\n",
       "      <td>0.770300</td>\n",
       "      <td>0.665400</td>\n",
       "      <td>0.882100</td>\n",
       "      <td>0.902400</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.917900</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.879200</td>\n",
       "      <td>0.957100</td>\n",
       "      <td>0.657800</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.573400</td>\n",
       "      <td>0.757100</td>\n",
       "      <td>0.783700</td>\n",
       "      <td>0.917600</td>\n",
       "      <td>0.908100</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.668700</td>\n",
       "      <td>0.865500</td>\n",
       "      <td>0.656700</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.807800</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.715300</td>\n",
       "      <td>0.918200</td>\n",
       "      <td>0.898700</td>\n",
       "      <td>0.953300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>17.474400</td>\n",
       "      <td>10.699405</td>\n",
       "      <td>0.726900</td>\n",
       "      <td>0.862200</td>\n",
       "      <td>0.808600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.744800</td>\n",
       "      <td>0.661700</td>\n",
       "      <td>0.861600</td>\n",
       "      <td>0.895100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.912200</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.857900</td>\n",
       "      <td>0.942900</td>\n",
       "      <td>0.621300</td>\n",
       "      <td>0.886700</td>\n",
       "      <td>0.538400</td>\n",
       "      <td>0.752400</td>\n",
       "      <td>0.813600</td>\n",
       "      <td>0.935300</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.944400</td>\n",
       "      <td>0.649800</td>\n",
       "      <td>0.844800</td>\n",
       "      <td>0.500700</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.796300</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>0.695400</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.883200</td>\n",
       "      <td>0.926700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>17.474400</td>\n",
       "      <td>10.485437</td>\n",
       "      <td>0.759800</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.834800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>0.773600</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>0.861400</td>\n",
       "      <td>0.889800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.904600</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.628000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.572200</td>\n",
       "      <td>0.752400</td>\n",
       "      <td>0.820600</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.910600</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.677900</td>\n",
       "      <td>0.855200</td>\n",
       "      <td>0.655000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.779400</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.767800</td>\n",
       "      <td>0.881800</td>\n",
       "      <td>0.884300</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>17.474400</td>\n",
       "      <td>10.379474</td>\n",
       "      <td>0.747300</td>\n",
       "      <td>0.878900</td>\n",
       "      <td>0.827600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.761800</td>\n",
       "      <td>0.667700</td>\n",
       "      <td>0.870500</td>\n",
       "      <td>0.898900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.914500</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.910100</td>\n",
       "      <td>0.942900</td>\n",
       "      <td>0.590600</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.588700</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>0.797400</td>\n",
       "      <td>0.917600</td>\n",
       "      <td>0.910500</td>\n",
       "      <td>0.944400</td>\n",
       "      <td>0.674600</td>\n",
       "      <td>0.858600</td>\n",
       "      <td>0.642500</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.812100</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.703500</td>\n",
       "      <td>0.918200</td>\n",
       "      <td>0.842700</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>17.474400</td>\n",
       "      <td>10.245391</td>\n",
       "      <td>0.757200</td>\n",
       "      <td>0.880100</td>\n",
       "      <td>0.826500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>0.771200</td>\n",
       "      <td>0.674100</td>\n",
       "      <td>0.865600</td>\n",
       "      <td>0.895100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.910500</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.911200</td>\n",
       "      <td>0.971400</td>\n",
       "      <td>0.667900</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.574700</td>\n",
       "      <td>0.752400</td>\n",
       "      <td>0.830600</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.820100</td>\n",
       "      <td>0.933300</td>\n",
       "      <td>0.678700</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>0.677100</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.817200</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.703700</td>\n",
       "      <td>0.890900</td>\n",
       "      <td>0.890700</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>17.474400</td>\n",
       "      <td>10.299535</td>\n",
       "      <td>0.738400</td>\n",
       "      <td>0.868700</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.754200</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>0.867300</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.909500</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.844200</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.647500</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.584100</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>0.783100</td>\n",
       "      <td>0.905900</td>\n",
       "      <td>0.902700</td>\n",
       "      <td>0.966700</td>\n",
       "      <td>0.676100</td>\n",
       "      <td>0.851700</td>\n",
       "      <td>0.584100</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.804800</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.714800</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.842600</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>17.474400</td>\n",
       "      <td>9.935101</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>0.897300</td>\n",
       "      <td>0.845600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.783600</td>\n",
       "      <td>0.668200</td>\n",
       "      <td>0.862700</td>\n",
       "      <td>0.881600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.896900</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.646300</td>\n",
       "      <td>0.866700</td>\n",
       "      <td>0.625200</td>\n",
       "      <td>0.747600</td>\n",
       "      <td>0.824100</td>\n",
       "      <td>0.917600</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.669400</td>\n",
       "      <td>0.813800</td>\n",
       "      <td>0.671100</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.831600</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.872700</td>\n",
       "      <td>0.867700</td>\n",
       "      <td>0.926700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>17.474400</td>\n",
       "      <td>10.013911</td>\n",
       "      <td>0.763500</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>0.830400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031900</td>\n",
       "      <td>0.777200</td>\n",
       "      <td>0.669700</td>\n",
       "      <td>0.867200</td>\n",
       "      <td>0.897100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.910400</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.711800</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.601300</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>0.869100</td>\n",
       "      <td>0.947100</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.641700</td>\n",
       "      <td>0.858600</td>\n",
       "      <td>0.632100</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.801400</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.703700</td>\n",
       "      <td>0.872700</td>\n",
       "      <td>0.842000</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>17.474400</td>\n",
       "      <td>9.795257</td>\n",
       "      <td>0.776500</td>\n",
       "      <td>0.912700</td>\n",
       "      <td>0.853900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.793000</td>\n",
       "      <td>0.675500</td>\n",
       "      <td>0.877100</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.913500</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.862200</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.686500</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.626100</td>\n",
       "      <td>0.776200</td>\n",
       "      <td>0.833100</td>\n",
       "      <td>0.929400</td>\n",
       "      <td>0.921900</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.705600</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>0.697200</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.826800</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.772600</td>\n",
       "      <td>0.881800</td>\n",
       "      <td>0.832900</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>15.403300</td>\n",
       "      <td>9.744800</td>\n",
       "      <td>0.782300</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.859400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.798200</td>\n",
       "      <td>0.679500</td>\n",
       "      <td>0.881500</td>\n",
       "      <td>0.894800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.910500</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.911900</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.742400</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.561900</td>\n",
       "      <td>0.742900</td>\n",
       "      <td>0.873400</td>\n",
       "      <td>0.941200</td>\n",
       "      <td>0.895900</td>\n",
       "      <td>0.966700</td>\n",
       "      <td>0.667600</td>\n",
       "      <td>0.851700</td>\n",
       "      <td>0.723200</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.754000</td>\n",
       "      <td>0.890900</td>\n",
       "      <td>0.842300</td>\n",
       "      <td>0.926700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>15.403300</td>\n",
       "      <td>9.932231</td>\n",
       "      <td>0.755200</td>\n",
       "      <td>0.882500</td>\n",
       "      <td>0.835800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.769900</td>\n",
       "      <td>0.678600</td>\n",
       "      <td>0.879800</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.907300</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.910100</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.666500</td>\n",
       "      <td>0.886700</td>\n",
       "      <td>0.603700</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>0.803900</td>\n",
       "      <td>0.929400</td>\n",
       "      <td>0.873600</td>\n",
       "      <td>0.922200</td>\n",
       "      <td>0.625500</td>\n",
       "      <td>0.875900</td>\n",
       "      <td>0.637500</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.824900</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.764300</td>\n",
       "      <td>0.890900</td>\n",
       "      <td>0.841900</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>15.403300</td>\n",
       "      <td>9.918054</td>\n",
       "      <td>0.751700</td>\n",
       "      <td>0.874000</td>\n",
       "      <td>0.812400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.765000</td>\n",
       "      <td>0.674800</td>\n",
       "      <td>0.870600</td>\n",
       "      <td>0.898300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.914500</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.818200</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.696600</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.526700</td>\n",
       "      <td>0.738100</td>\n",
       "      <td>0.828600</td>\n",
       "      <td>0.941200</td>\n",
       "      <td>0.884300</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.599600</td>\n",
       "      <td>0.851700</td>\n",
       "      <td>0.726200</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.852900</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.738800</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.845200</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>15.403300</td>\n",
       "      <td>9.862332</td>\n",
       "      <td>0.762100</td>\n",
       "      <td>0.901300</td>\n",
       "      <td>0.830300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045600</td>\n",
       "      <td>0.778200</td>\n",
       "      <td>0.669200</td>\n",
       "      <td>0.870800</td>\n",
       "      <td>0.891100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.906700</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.896500</td>\n",
       "      <td>0.957100</td>\n",
       "      <td>0.690800</td>\n",
       "      <td>0.873300</td>\n",
       "      <td>0.582200</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>0.818100</td>\n",
       "      <td>0.929400</td>\n",
       "      <td>0.879200</td>\n",
       "      <td>0.944400</td>\n",
       "      <td>0.665600</td>\n",
       "      <td>0.837900</td>\n",
       "      <td>0.661800</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.844500</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.724500</td>\n",
       "      <td>0.890900</td>\n",
       "      <td>0.857300</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>15.403300</td>\n",
       "      <td>9.849643</td>\n",
       "      <td>0.763700</td>\n",
       "      <td>0.895400</td>\n",
       "      <td>0.831900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.780600</td>\n",
       "      <td>0.676800</td>\n",
       "      <td>0.872200</td>\n",
       "      <td>0.900600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.915200</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.820700</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.700600</td>\n",
       "      <td>0.886700</td>\n",
       "      <td>0.562400</td>\n",
       "      <td>0.747600</td>\n",
       "      <td>0.856500</td>\n",
       "      <td>0.958800</td>\n",
       "      <td>0.895200</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.675700</td>\n",
       "      <td>0.851700</td>\n",
       "      <td>0.654600</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.863100</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.758700</td>\n",
       "      <td>0.936400</td>\n",
       "      <td>0.849900</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>15.403300</td>\n",
       "      <td>9.756020</td>\n",
       "      <td>0.764100</td>\n",
       "      <td>0.897800</td>\n",
       "      <td>0.839500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059800</td>\n",
       "      <td>0.778500</td>\n",
       "      <td>0.666200</td>\n",
       "      <td>0.876800</td>\n",
       "      <td>0.902300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.916400</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.875600</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.906700</td>\n",
       "      <td>0.578200</td>\n",
       "      <td>0.747600</td>\n",
       "      <td>0.821800</td>\n",
       "      <td>0.929400</td>\n",
       "      <td>0.864500</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.661000</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>0.731900</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.840800</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.715700</td>\n",
       "      <td>0.918200</td>\n",
       "      <td>0.851200</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>15.403300</td>\n",
       "      <td>9.716172</td>\n",
       "      <td>0.765800</td>\n",
       "      <td>0.902800</td>\n",
       "      <td>0.844800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.781300</td>\n",
       "      <td>0.657800</td>\n",
       "      <td>0.865800</td>\n",
       "      <td>0.892200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.907500</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.839800</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.745400</td>\n",
       "      <td>0.906700</td>\n",
       "      <td>0.586100</td>\n",
       "      <td>0.747600</td>\n",
       "      <td>0.820600</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.831000</td>\n",
       "      <td>0.922200</td>\n",
       "      <td>0.645800</td>\n",
       "      <td>0.848300</td>\n",
       "      <td>0.792100</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.823200</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.710200</td>\n",
       "      <td>0.890900</td>\n",
       "      <td>0.864100</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>15.403300</td>\n",
       "      <td>9.603412</td>\n",
       "      <td>0.758600</td>\n",
       "      <td>0.877700</td>\n",
       "      <td>0.825400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055200</td>\n",
       "      <td>0.773600</td>\n",
       "      <td>0.679400</td>\n",
       "      <td>0.876800</td>\n",
       "      <td>0.898000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.912900</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.767400</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.706400</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.594900</td>\n",
       "      <td>0.757100</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.941200</td>\n",
       "      <td>0.881100</td>\n",
       "      <td>0.966700</td>\n",
       "      <td>0.637500</td>\n",
       "      <td>0.844800</td>\n",
       "      <td>0.738600</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.829100</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.761500</td>\n",
       "      <td>0.872700</td>\n",
       "      <td>0.848200</td>\n",
       "      <td>0.953300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>15.403300</td>\n",
       "      <td>9.660398</td>\n",
       "      <td>0.767800</td>\n",
       "      <td>0.906100</td>\n",
       "      <td>0.841100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.782200</td>\n",
       "      <td>0.665600</td>\n",
       "      <td>0.870600</td>\n",
       "      <td>0.894600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.909700</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.870600</td>\n",
       "      <td>0.942900</td>\n",
       "      <td>0.728800</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.611500</td>\n",
       "      <td>0.766700</td>\n",
       "      <td>0.825800</td>\n",
       "      <td>0.917600</td>\n",
       "      <td>0.827600</td>\n",
       "      <td>0.933300</td>\n",
       "      <td>0.632100</td>\n",
       "      <td>0.855200</td>\n",
       "      <td>0.750200</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.804500</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.793200</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.833600</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>15.403300</td>\n",
       "      <td>9.487251</td>\n",
       "      <td>0.765300</td>\n",
       "      <td>0.893000</td>\n",
       "      <td>0.831800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061600</td>\n",
       "      <td>0.780900</td>\n",
       "      <td>0.675100</td>\n",
       "      <td>0.877000</td>\n",
       "      <td>0.901800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.915700</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.786800</td>\n",
       "      <td>0.957100</td>\n",
       "      <td>0.644500</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.600800</td>\n",
       "      <td>0.766700</td>\n",
       "      <td>0.827100</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.935600</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.676900</td>\n",
       "      <td>0.872400</td>\n",
       "      <td>0.750800</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.831800</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.775300</td>\n",
       "      <td>0.890900</td>\n",
       "      <td>0.823500</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>15.403300</td>\n",
       "      <td>9.425553</td>\n",
       "      <td>0.783600</td>\n",
       "      <td>0.912600</td>\n",
       "      <td>0.861700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.798300</td>\n",
       "      <td>0.673300</td>\n",
       "      <td>0.881100</td>\n",
       "      <td>0.899300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.896400</td>\n",
       "      <td>0.957100</td>\n",
       "      <td>0.748300</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.600900</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>0.817800</td>\n",
       "      <td>0.941200</td>\n",
       "      <td>0.919100</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.633100</td>\n",
       "      <td>0.851700</td>\n",
       "      <td>0.791500</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.830700</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.768500</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>14.270100</td>\n",
       "      <td>9.536987</td>\n",
       "      <td>0.771600</td>\n",
       "      <td>0.902600</td>\n",
       "      <td>0.849000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>0.785700</td>\n",
       "      <td>0.669600</td>\n",
       "      <td>0.873900</td>\n",
       "      <td>0.895900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.910900</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.881300</td>\n",
       "      <td>0.957100</td>\n",
       "      <td>0.682600</td>\n",
       "      <td>0.886700</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>0.806100</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.849400</td>\n",
       "      <td>0.922200</td>\n",
       "      <td>0.614900</td>\n",
       "      <td>0.875900</td>\n",
       "      <td>0.770300</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.818500</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>0.805800</td>\n",
       "      <td>0.927300</td>\n",
       "      <td>0.876300</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>14.270100</td>\n",
       "      <td>9.383119</td>\n",
       "      <td>0.775300</td>\n",
       "      <td>0.905600</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.791800</td>\n",
       "      <td>0.675900</td>\n",
       "      <td>0.868400</td>\n",
       "      <td>0.895600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.911300</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.847700</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.622100</td>\n",
       "      <td>0.866700</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>0.742900</td>\n",
       "      <td>0.820900</td>\n",
       "      <td>0.935300</td>\n",
       "      <td>0.884100</td>\n",
       "      <td>0.944400</td>\n",
       "      <td>0.612000</td>\n",
       "      <td>0.865500</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.851500</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.847200</td>\n",
       "      <td>0.936400</td>\n",
       "      <td>0.878000</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>14.270100</td>\n",
       "      <td>9.548071</td>\n",
       "      <td>0.791900</td>\n",
       "      <td>0.928100</td>\n",
       "      <td>0.874600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.806500</td>\n",
       "      <td>0.664300</td>\n",
       "      <td>0.866200</td>\n",
       "      <td>0.895200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.730900</td>\n",
       "      <td>0.866700</td>\n",
       "      <td>0.609500</td>\n",
       "      <td>0.752400</td>\n",
       "      <td>0.815800</td>\n",
       "      <td>0.929400</td>\n",
       "      <td>0.931200</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.668900</td>\n",
       "      <td>0.855200</td>\n",
       "      <td>0.804500</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.831400</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.758000</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.856600</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>14.270100</td>\n",
       "      <td>9.256506</td>\n",
       "      <td>0.783500</td>\n",
       "      <td>0.913300</td>\n",
       "      <td>0.851000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>0.686600</td>\n",
       "      <td>0.878500</td>\n",
       "      <td>0.895800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.910900</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.831200</td>\n",
       "      <td>0.942900</td>\n",
       "      <td>0.639600</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.597700</td>\n",
       "      <td>0.742900</td>\n",
       "      <td>0.831200</td>\n",
       "      <td>0.941200</td>\n",
       "      <td>0.910100</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.669500</td>\n",
       "      <td>0.865500</td>\n",
       "      <td>0.754600</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.857400</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.845800</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.897900</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>14.270100</td>\n",
       "      <td>9.176326</td>\n",
       "      <td>0.800800</td>\n",
       "      <td>0.934500</td>\n",
       "      <td>0.868800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.814700</td>\n",
       "      <td>0.687800</td>\n",
       "      <td>0.872800</td>\n",
       "      <td>0.893400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.909000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.755100</td>\n",
       "      <td>0.886700</td>\n",
       "      <td>0.592300</td>\n",
       "      <td>0.738100</td>\n",
       "      <td>0.832900</td>\n",
       "      <td>0.941200</td>\n",
       "      <td>0.904200</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.656100</td>\n",
       "      <td>0.841400</td>\n",
       "      <td>0.740900</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.856700</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.867800</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.892200</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>14.270100</td>\n",
       "      <td>9.248139</td>\n",
       "      <td>0.777800</td>\n",
       "      <td>0.903100</td>\n",
       "      <td>0.851500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038200</td>\n",
       "      <td>0.791800</td>\n",
       "      <td>0.687400</td>\n",
       "      <td>0.873800</td>\n",
       "      <td>0.897100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.911600</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.912100</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.708000</td>\n",
       "      <td>0.873300</td>\n",
       "      <td>0.588200</td>\n",
       "      <td>0.766700</td>\n",
       "      <td>0.845600</td>\n",
       "      <td>0.941200</td>\n",
       "      <td>0.876500</td>\n",
       "      <td>0.966700</td>\n",
       "      <td>0.649700</td>\n",
       "      <td>0.844800</td>\n",
       "      <td>0.681400</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.837800</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.779400</td>\n",
       "      <td>0.918200</td>\n",
       "      <td>0.899000</td>\n",
       "      <td>0.953300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>14.270100</td>\n",
       "      <td>9.273214</td>\n",
       "      <td>0.784800</td>\n",
       "      <td>0.916200</td>\n",
       "      <td>0.863500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.800500</td>\n",
       "      <td>0.686100</td>\n",
       "      <td>0.872200</td>\n",
       "      <td>0.893100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.906400</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.789900</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.739600</td>\n",
       "      <td>0.886700</td>\n",
       "      <td>0.600200</td>\n",
       "      <td>0.766700</td>\n",
       "      <td>0.844300</td>\n",
       "      <td>0.917600</td>\n",
       "      <td>0.903500</td>\n",
       "      <td>0.944400</td>\n",
       "      <td>0.684600</td>\n",
       "      <td>0.858600</td>\n",
       "      <td>0.774300</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.838200</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.788600</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.884400</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>14.270100</td>\n",
       "      <td>9.161734</td>\n",
       "      <td>0.791100</td>\n",
       "      <td>0.915300</td>\n",
       "      <td>0.862700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>0.806900</td>\n",
       "      <td>0.685600</td>\n",
       "      <td>0.870800</td>\n",
       "      <td>0.895200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.909900</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.909900</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.726500</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.603200</td>\n",
       "      <td>0.747600</td>\n",
       "      <td>0.841500</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.885500</td>\n",
       "      <td>0.966700</td>\n",
       "      <td>0.669000</td>\n",
       "      <td>0.858600</td>\n",
       "      <td>0.753300</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.826100</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.817400</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.878400</td>\n",
       "      <td>0.953300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>14.270100</td>\n",
       "      <td>8.972221</td>\n",
       "      <td>0.794100</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.870100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040600</td>\n",
       "      <td>0.810200</td>\n",
       "      <td>0.682800</td>\n",
       "      <td>0.883100</td>\n",
       "      <td>0.899800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.914000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.805800</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.633300</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.630500</td>\n",
       "      <td>0.752400</td>\n",
       "      <td>0.857900</td>\n",
       "      <td>0.929400</td>\n",
       "      <td>0.962800</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.711300</td>\n",
       "      <td>0.848300</td>\n",
       "      <td>0.794300</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.837700</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.834300</td>\n",
       "      <td>0.918200</td>\n",
       "      <td>0.872900</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>14.270100</td>\n",
       "      <td>9.220481</td>\n",
       "      <td>0.793600</td>\n",
       "      <td>0.929400</td>\n",
       "      <td>0.861000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>0.808700</td>\n",
       "      <td>0.680300</td>\n",
       "      <td>0.869800</td>\n",
       "      <td>0.890700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.905100</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.911900</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.735200</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.609700</td>\n",
       "      <td>0.738100</td>\n",
       "      <td>0.847900</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.886200</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.672400</td>\n",
       "      <td>0.851700</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.858100</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.771800</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.885800</td>\n",
       "      <td>0.926700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>14.270100</td>\n",
       "      <td>9.028274</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.934000</td>\n",
       "      <td>0.870500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.817500</td>\n",
       "      <td>0.693300</td>\n",
       "      <td>0.875600</td>\n",
       "      <td>0.895000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.909200</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.907300</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.741600</td>\n",
       "      <td>0.866700</td>\n",
       "      <td>0.610100</td>\n",
       "      <td>0.752400</td>\n",
       "      <td>0.825300</td>\n",
       "      <td>0.929400</td>\n",
       "      <td>0.898500</td>\n",
       "      <td>0.966700</td>\n",
       "      <td>0.671600</td>\n",
       "      <td>0.858600</td>\n",
       "      <td>0.758400</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.856100</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.853000</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>14.270100</td>\n",
       "      <td>9.270720</td>\n",
       "      <td>0.786900</td>\n",
       "      <td>0.929300</td>\n",
       "      <td>0.864500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.673100</td>\n",
       "      <td>0.870800</td>\n",
       "      <td>0.892700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.907900</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.911900</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.717400</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.627400</td>\n",
       "      <td>0.742900</td>\n",
       "      <td>0.826800</td>\n",
       "      <td>0.917600</td>\n",
       "      <td>0.879800</td>\n",
       "      <td>0.966700</td>\n",
       "      <td>0.658700</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>0.758400</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.840700</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.872700</td>\n",
       "      <td>0.878300</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>13.381800</td>\n",
       "      <td>9.093452</td>\n",
       "      <td>0.793600</td>\n",
       "      <td>0.926000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031800</td>\n",
       "      <td>0.807800</td>\n",
       "      <td>0.684800</td>\n",
       "      <td>0.880900</td>\n",
       "      <td>0.900400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.914800</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.910100</td>\n",
       "      <td>0.942900</td>\n",
       "      <td>0.727300</td>\n",
       "      <td>0.886700</td>\n",
       "      <td>0.601700</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>0.860900</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.949700</td>\n",
       "      <td>0.966700</td>\n",
       "      <td>0.679600</td>\n",
       "      <td>0.875900</td>\n",
       "      <td>0.732900</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.854100</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.744100</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>13.381800</td>\n",
       "      <td>9.203836</td>\n",
       "      <td>0.794700</td>\n",
       "      <td>0.931100</td>\n",
       "      <td>0.876600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.671900</td>\n",
       "      <td>0.871400</td>\n",
       "      <td>0.894100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.909500</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.899300</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.752100</td>\n",
       "      <td>0.873300</td>\n",
       "      <td>0.609200</td>\n",
       "      <td>0.757100</td>\n",
       "      <td>0.849100</td>\n",
       "      <td>0.917600</td>\n",
       "      <td>0.951500</td>\n",
       "      <td>0.966700</td>\n",
       "      <td>0.658600</td>\n",
       "      <td>0.865500</td>\n",
       "      <td>0.792200</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.842600</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.724700</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.867800</td>\n",
       "      <td>0.926700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>13.381800</td>\n",
       "      <td>9.099826</td>\n",
       "      <td>0.803200</td>\n",
       "      <td>0.942400</td>\n",
       "      <td>0.888200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031400</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.672300</td>\n",
       "      <td>0.879700</td>\n",
       "      <td>0.907300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.922200</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.895600</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.764300</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.611100</td>\n",
       "      <td>0.757100</td>\n",
       "      <td>0.856300</td>\n",
       "      <td>0.935300</td>\n",
       "      <td>0.951200</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.681800</td>\n",
       "      <td>0.858600</td>\n",
       "      <td>0.796700</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.840700</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.750300</td>\n",
       "      <td>0.918200</td>\n",
       "      <td>0.883600</td>\n",
       "      <td>0.953300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>13.381800</td>\n",
       "      <td>9.142688</td>\n",
       "      <td>0.791800</td>\n",
       "      <td>0.935200</td>\n",
       "      <td>0.873500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028800</td>\n",
       "      <td>0.805800</td>\n",
       "      <td>0.670400</td>\n",
       "      <td>0.878100</td>\n",
       "      <td>0.898700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.910100</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.743600</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.612400</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>0.829800</td>\n",
       "      <td>0.929400</td>\n",
       "      <td>0.949400</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.664700</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>0.771700</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.836100</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.742200</td>\n",
       "      <td>0.918200</td>\n",
       "      <td>0.858300</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>13.381800</td>\n",
       "      <td>9.120647</td>\n",
       "      <td>0.796600</td>\n",
       "      <td>0.931900</td>\n",
       "      <td>0.882400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.810600</td>\n",
       "      <td>0.670700</td>\n",
       "      <td>0.874900</td>\n",
       "      <td>0.891900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.907400</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.909900</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.751900</td>\n",
       "      <td>0.866700</td>\n",
       "      <td>0.607300</td>\n",
       "      <td>0.757100</td>\n",
       "      <td>0.839200</td>\n",
       "      <td>0.929400</td>\n",
       "      <td>0.949400</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.687000</td>\n",
       "      <td>0.858600</td>\n",
       "      <td>0.760900</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.848200</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.760100</td>\n",
       "      <td>0.881800</td>\n",
       "      <td>0.851900</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>13.381800</td>\n",
       "      <td>8.966622</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.930200</td>\n",
       "      <td>0.882900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>0.815700</td>\n",
       "      <td>0.684700</td>\n",
       "      <td>0.879000</td>\n",
       "      <td>0.902400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.917400</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.893200</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.737200</td>\n",
       "      <td>0.906700</td>\n",
       "      <td>0.600300</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>0.851300</td>\n",
       "      <td>0.947100</td>\n",
       "      <td>0.965900</td>\n",
       "      <td>0.988900</td>\n",
       "      <td>0.695800</td>\n",
       "      <td>0.865500</td>\n",
       "      <td>0.747600</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.847400</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.784400</td>\n",
       "      <td>0.881800</td>\n",
       "      <td>0.887200</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>13.381800</td>\n",
       "      <td>8.924029</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>0.927700</td>\n",
       "      <td>0.876400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.813400</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.883200</td>\n",
       "      <td>0.897600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.912400</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.866300</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.726700</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.608300</td>\n",
       "      <td>0.752400</td>\n",
       "      <td>0.842700</td>\n",
       "      <td>0.917600</td>\n",
       "      <td>0.950400</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.683400</td>\n",
       "      <td>0.858600</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.846100</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.794700</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.867600</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>13.381800</td>\n",
       "      <td>8.903287</td>\n",
       "      <td>0.801700</td>\n",
       "      <td>0.925800</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030100</td>\n",
       "      <td>0.817600</td>\n",
       "      <td>0.679500</td>\n",
       "      <td>0.875100</td>\n",
       "      <td>0.893900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.908900</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.917800</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.755300</td>\n",
       "      <td>0.886700</td>\n",
       "      <td>0.598000</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>0.839700</td>\n",
       "      <td>0.935300</td>\n",
       "      <td>0.962600</td>\n",
       "      <td>0.966700</td>\n",
       "      <td>0.682700</td>\n",
       "      <td>0.851700</td>\n",
       "      <td>0.754100</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>0.881800</td>\n",
       "      <td>0.856200</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>13.381800</td>\n",
       "      <td>8.958597</td>\n",
       "      <td>0.799400</td>\n",
       "      <td>0.930200</td>\n",
       "      <td>0.876900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>0.814300</td>\n",
       "      <td>0.678400</td>\n",
       "      <td>0.880400</td>\n",
       "      <td>0.899100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.913800</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.912300</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.736700</td>\n",
       "      <td>0.906700</td>\n",
       "      <td>0.595800</td>\n",
       "      <td>0.752400</td>\n",
       "      <td>0.842100</td>\n",
       "      <td>0.947100</td>\n",
       "      <td>0.949400</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.658000</td>\n",
       "      <td>0.865500</td>\n",
       "      <td>0.791000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.857500</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.762100</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.889000</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>13.381800</td>\n",
       "      <td>8.888498</td>\n",
       "      <td>0.797500</td>\n",
       "      <td>0.928800</td>\n",
       "      <td>0.873500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>0.813100</td>\n",
       "      <td>0.686700</td>\n",
       "      <td>0.878500</td>\n",
       "      <td>0.902600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.917800</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.912100</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.757300</td>\n",
       "      <td>0.913300</td>\n",
       "      <td>0.595600</td>\n",
       "      <td>0.747600</td>\n",
       "      <td>0.840200</td>\n",
       "      <td>0.941200</td>\n",
       "      <td>0.940400</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>0.755800</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.862300</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.784900</td>\n",
       "      <td>0.918200</td>\n",
       "      <td>0.856600</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>13.381800</td>\n",
       "      <td>8.814480</td>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.929500</td>\n",
       "      <td>0.881800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>0.818000</td>\n",
       "      <td>0.685300</td>\n",
       "      <td>0.881200</td>\n",
       "      <td>0.896700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.911300</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.881700</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.753600</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.616900</td>\n",
       "      <td>0.747600</td>\n",
       "      <td>0.848300</td>\n",
       "      <td>0.935300</td>\n",
       "      <td>0.950700</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.681100</td>\n",
       "      <td>0.879300</td>\n",
       "      <td>0.762200</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.877700</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>13.247600</td>\n",
       "      <td>8.794025</td>\n",
       "      <td>0.806200</td>\n",
       "      <td>0.929800</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.682300</td>\n",
       "      <td>0.884200</td>\n",
       "      <td>0.902300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.916600</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.910700</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.736200</td>\n",
       "      <td>0.906700</td>\n",
       "      <td>0.609300</td>\n",
       "      <td>0.757100</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>0.929400</td>\n",
       "      <td>0.951300</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.690600</td>\n",
       "      <td>0.865500</td>\n",
       "      <td>0.799300</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.868300</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.794300</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.858600</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>13.247600</td>\n",
       "      <td>8.853990</td>\n",
       "      <td>0.802800</td>\n",
       "      <td>0.925700</td>\n",
       "      <td>0.870900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.816600</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>0.886000</td>\n",
       "      <td>0.902200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.917600</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.913000</td>\n",
       "      <td>0.942900</td>\n",
       "      <td>0.727000</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.599600</td>\n",
       "      <td>0.752400</td>\n",
       "      <td>0.847800</td>\n",
       "      <td>0.935300</td>\n",
       "      <td>0.951000</td>\n",
       "      <td>0.966700</td>\n",
       "      <td>0.674400</td>\n",
       "      <td>0.865500</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.864600</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.805400</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.876800</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>13.247600</td>\n",
       "      <td>8.807178</td>\n",
       "      <td>0.794600</td>\n",
       "      <td>0.922000</td>\n",
       "      <td>0.858900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.809300</td>\n",
       "      <td>0.687800</td>\n",
       "      <td>0.885800</td>\n",
       "      <td>0.904300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.918700</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.716200</td>\n",
       "      <td>0.906700</td>\n",
       "      <td>0.604500</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>0.844000</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.891100</td>\n",
       "      <td>0.944400</td>\n",
       "      <td>0.661200</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.859800</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.798300</td>\n",
       "      <td>0.927300</td>\n",
       "      <td>0.875900</td>\n",
       "      <td>0.953300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>13.247600</td>\n",
       "      <td>8.908164</td>\n",
       "      <td>0.796200</td>\n",
       "      <td>0.922100</td>\n",
       "      <td>0.867900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.810500</td>\n",
       "      <td>0.682700</td>\n",
       "      <td>0.873700</td>\n",
       "      <td>0.897100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.911500</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.909900</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.729000</td>\n",
       "      <td>0.906700</td>\n",
       "      <td>0.598100</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>0.824500</td>\n",
       "      <td>0.935300</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.650100</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>0.799900</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.858400</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.802600</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>13.247600</td>\n",
       "      <td>8.862531</td>\n",
       "      <td>0.793900</td>\n",
       "      <td>0.926200</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041300</td>\n",
       "      <td>0.807800</td>\n",
       "      <td>0.680200</td>\n",
       "      <td>0.877700</td>\n",
       "      <td>0.902000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.915800</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.914000</td>\n",
       "      <td>0.942900</td>\n",
       "      <td>0.742100</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.597400</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>0.834400</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.892900</td>\n",
       "      <td>0.966700</td>\n",
       "      <td>0.655500</td>\n",
       "      <td>0.858600</td>\n",
       "      <td>0.814100</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.863100</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.765000</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.860800</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>13.247600</td>\n",
       "      <td>8.798493</td>\n",
       "      <td>0.799500</td>\n",
       "      <td>0.930700</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>0.813900</td>\n",
       "      <td>0.681700</td>\n",
       "      <td>0.874500</td>\n",
       "      <td>0.895800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.910100</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.914400</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.754000</td>\n",
       "      <td>0.886700</td>\n",
       "      <td>0.604200</td>\n",
       "      <td>0.757100</td>\n",
       "      <td>0.835700</td>\n",
       "      <td>0.935300</td>\n",
       "      <td>0.942600</td>\n",
       "      <td>0.966700</td>\n",
       "      <td>0.678600</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>0.762400</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.868900</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.770800</td>\n",
       "      <td>0.890900</td>\n",
       "      <td>0.863700</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>13.247600</td>\n",
       "      <td>8.847508</td>\n",
       "      <td>0.791900</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>0.859400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050500</td>\n",
       "      <td>0.805600</td>\n",
       "      <td>0.683600</td>\n",
       "      <td>0.885500</td>\n",
       "      <td>0.896600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.910400</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.731000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.606300</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>0.840200</td>\n",
       "      <td>0.929400</td>\n",
       "      <td>0.881800</td>\n",
       "      <td>0.944400</td>\n",
       "      <td>0.659000</td>\n",
       "      <td>0.855200</td>\n",
       "      <td>0.780200</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.854600</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.798600</td>\n",
       "      <td>0.918200</td>\n",
       "      <td>0.854900</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>13.247600</td>\n",
       "      <td>8.715141</td>\n",
       "      <td>0.803800</td>\n",
       "      <td>0.929000</td>\n",
       "      <td>0.876800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.819600</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.883700</td>\n",
       "      <td>0.900500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.915500</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.910800</td>\n",
       "      <td>0.957100</td>\n",
       "      <td>0.731800</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>0.845500</td>\n",
       "      <td>0.929400</td>\n",
       "      <td>0.911600</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.683900</td>\n",
       "      <td>0.865500</td>\n",
       "      <td>0.819100</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.863800</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.802700</td>\n",
       "      <td>0.890900</td>\n",
       "      <td>0.871700</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>13.247600</td>\n",
       "      <td>8.927250</td>\n",
       "      <td>0.795300</td>\n",
       "      <td>0.929700</td>\n",
       "      <td>0.876500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.809100</td>\n",
       "      <td>0.669900</td>\n",
       "      <td>0.876600</td>\n",
       "      <td>0.892100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.906800</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.909900</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.737300</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.592400</td>\n",
       "      <td>0.752400</td>\n",
       "      <td>0.822200</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.951100</td>\n",
       "      <td>0.966700</td>\n",
       "      <td>0.664200</td>\n",
       "      <td>0.865500</td>\n",
       "      <td>0.785900</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.859400</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.773700</td>\n",
       "      <td>0.890900</td>\n",
       "      <td>0.857100</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>13.247600</td>\n",
       "      <td>8.846444</td>\n",
       "      <td>0.789400</td>\n",
       "      <td>0.922500</td>\n",
       "      <td>0.867600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>0.804400</td>\n",
       "      <td>0.682900</td>\n",
       "      <td>0.883900</td>\n",
       "      <td>0.899300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.913800</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.910400</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.748100</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.589600</td>\n",
       "      <td>0.766700</td>\n",
       "      <td>0.838800</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.911600</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.675200</td>\n",
       "      <td>0.865500</td>\n",
       "      <td>0.763000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.854500</td>\n",
       "      <td>0.936000</td>\n",
       "      <td>0.756700</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.846500</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>13.247600</td>\n",
       "      <td>8.792347</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.866700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>0.812800</td>\n",
       "      <td>0.681600</td>\n",
       "      <td>0.885000</td>\n",
       "      <td>0.901700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.915600</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.911800</td>\n",
       "      <td>0.942900</td>\n",
       "      <td>0.727600</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.608000</td>\n",
       "      <td>0.766700</td>\n",
       "      <td>0.834100</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.898200</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.660500</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>0.812300</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.854200</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.810600</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.873200</td>\n",
       "      <td>0.953300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>13.043700</td>\n",
       "      <td>8.819119</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.923600</td>\n",
       "      <td>0.856400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.682600</td>\n",
       "      <td>0.876600</td>\n",
       "      <td>0.897500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.912400</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.910800</td>\n",
       "      <td>0.942900</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.601400</td>\n",
       "      <td>0.757100</td>\n",
       "      <td>0.834800</td>\n",
       "      <td>0.935300</td>\n",
       "      <td>0.879200</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.649100</td>\n",
       "      <td>0.865500</td>\n",
       "      <td>0.777500</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.795800</td>\n",
       "      <td>0.890900</td>\n",
       "      <td>0.868300</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>13.043700</td>\n",
       "      <td>8.773153</td>\n",
       "      <td>0.802500</td>\n",
       "      <td>0.935400</td>\n",
       "      <td>0.891500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.817800</td>\n",
       "      <td>0.681100</td>\n",
       "      <td>0.872300</td>\n",
       "      <td>0.891900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.906000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.909900</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.749800</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.609400</td>\n",
       "      <td>0.752400</td>\n",
       "      <td>0.844300</td>\n",
       "      <td>0.941200</td>\n",
       "      <td>0.950700</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.676100</td>\n",
       "      <td>0.855200</td>\n",
       "      <td>0.771700</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.866600</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.769100</td>\n",
       "      <td>0.863600</td>\n",
       "      <td>0.877100</td>\n",
       "      <td>0.953300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>13.043700</td>\n",
       "      <td>8.773835</td>\n",
       "      <td>0.794100</td>\n",
       "      <td>0.920300</td>\n",
       "      <td>0.861700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029200</td>\n",
       "      <td>0.808600</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>0.888300</td>\n",
       "      <td>0.901700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.916600</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.912600</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.752700</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.606300</td>\n",
       "      <td>0.757100</td>\n",
       "      <td>0.841800</td>\n",
       "      <td>0.935300</td>\n",
       "      <td>0.900200</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.665500</td>\n",
       "      <td>0.865500</td>\n",
       "      <td>0.759700</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.861900</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.780300</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.860300</td>\n",
       "      <td>0.933300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>13.043700</td>\n",
       "      <td>8.801734</td>\n",
       "      <td>0.792000</td>\n",
       "      <td>0.921600</td>\n",
       "      <td>0.859600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.806000</td>\n",
       "      <td>0.678500</td>\n",
       "      <td>0.881400</td>\n",
       "      <td>0.893800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.908500</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.910700</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.722600</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.611700</td>\n",
       "      <td>0.752400</td>\n",
       "      <td>0.825700</td>\n",
       "      <td>0.917600</td>\n",
       "      <td>0.896200</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.655300</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>0.778800</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.860300</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.788600</td>\n",
       "      <td>0.890900</td>\n",
       "      <td>0.870400</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>13.043700</td>\n",
       "      <td>8.780231</td>\n",
       "      <td>0.801700</td>\n",
       "      <td>0.928900</td>\n",
       "      <td>0.869900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033400</td>\n",
       "      <td>0.815400</td>\n",
       "      <td>0.678300</td>\n",
       "      <td>0.881700</td>\n",
       "      <td>0.903700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.918200</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.912700</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.734500</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.610700</td>\n",
       "      <td>0.766700</td>\n",
       "      <td>0.837700</td>\n",
       "      <td>0.941200</td>\n",
       "      <td>0.902900</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.660900</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>0.817600</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.864700</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.795300</td>\n",
       "      <td>0.881800</td>\n",
       "      <td>0.879900</td>\n",
       "      <td>0.953300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>13.043700</td>\n",
       "      <td>8.826581</td>\n",
       "      <td>0.795200</td>\n",
       "      <td>0.922400</td>\n",
       "      <td>0.860900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.808700</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>0.882900</td>\n",
       "      <td>0.904700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.917700</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.913200</td>\n",
       "      <td>0.942900</td>\n",
       "      <td>0.746600</td>\n",
       "      <td>0.913300</td>\n",
       "      <td>0.607000</td>\n",
       "      <td>0.776200</td>\n",
       "      <td>0.841500</td>\n",
       "      <td>0.941200</td>\n",
       "      <td>0.872300</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.644600</td>\n",
       "      <td>0.855200</td>\n",
       "      <td>0.797200</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.795900</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>13.043700</td>\n",
       "      <td>8.786565</td>\n",
       "      <td>0.801700</td>\n",
       "      <td>0.930100</td>\n",
       "      <td>0.878600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>0.816500</td>\n",
       "      <td>0.681500</td>\n",
       "      <td>0.878800</td>\n",
       "      <td>0.897500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.911300</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.909900</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>0.753000</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>0.841500</td>\n",
       "      <td>0.947100</td>\n",
       "      <td>0.950700</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.663200</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>0.748400</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.871400</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.799600</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.875300</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>13.043700</td>\n",
       "      <td>8.793381</td>\n",
       "      <td>0.794300</td>\n",
       "      <td>0.919200</td>\n",
       "      <td>0.864300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.808600</td>\n",
       "      <td>0.685700</td>\n",
       "      <td>0.881300</td>\n",
       "      <td>0.897200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.911600</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.912700</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.729200</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.611300</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>0.857300</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.878500</td>\n",
       "      <td>0.966700</td>\n",
       "      <td>0.657000</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>0.749900</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.866700</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.803500</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.877000</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>13.043700</td>\n",
       "      <td>8.850237</td>\n",
       "      <td>0.786600</td>\n",
       "      <td>0.918700</td>\n",
       "      <td>0.848500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037000</td>\n",
       "      <td>0.800700</td>\n",
       "      <td>0.681900</td>\n",
       "      <td>0.875900</td>\n",
       "      <td>0.891700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.906100</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.726200</td>\n",
       "      <td>0.886700</td>\n",
       "      <td>0.618700</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>0.840900</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.876900</td>\n",
       "      <td>0.944400</td>\n",
       "      <td>0.646200</td>\n",
       "      <td>0.862100</td>\n",
       "      <td>0.743100</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.859200</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>0.783100</td>\n",
       "      <td>0.890900</td>\n",
       "      <td>0.859500</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>13.043700</td>\n",
       "      <td>8.850290</td>\n",
       "      <td>0.798500</td>\n",
       "      <td>0.927200</td>\n",
       "      <td>0.869800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.813500</td>\n",
       "      <td>0.683600</td>\n",
       "      <td>0.884100</td>\n",
       "      <td>0.898100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.912700</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.910500</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>0.729100</td>\n",
       "      <td>0.886700</td>\n",
       "      <td>0.605600</td>\n",
       "      <td>0.771400</td>\n",
       "      <td>0.845200</td>\n",
       "      <td>0.929400</td>\n",
       "      <td>0.949400</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.665900</td>\n",
       "      <td>0.865500</td>\n",
       "      <td>0.751300</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.852900</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.798500</td>\n",
       "      <td>0.881800</td>\n",
       "      <td>0.876800</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['class_embed.0.weight', 'class_embed.0.bias', 'class_embed.1.weight', 'class_embed.1.bias', 'class_embed.2.weight', 'class_embed.2.bias', 'class_embed.3.weight', 'class_embed.3.bias', 'class_embed.4.weight', 'class_embed.4.bias', 'class_embed.5.weight', 'class_embed.5.bias', 'bbox_embed.0.layers.0.weight', 'bbox_embed.0.layers.0.bias', 'bbox_embed.0.layers.1.weight', 'bbox_embed.0.layers.1.bias', 'bbox_embed.0.layers.2.weight', 'bbox_embed.0.layers.2.bias', 'bbox_embed.1.layers.0.weight', 'bbox_embed.1.layers.0.bias', 'bbox_embed.1.layers.1.weight', 'bbox_embed.1.layers.1.bias', 'bbox_embed.1.layers.2.weight', 'bbox_embed.1.layers.2.bias', 'bbox_embed.2.layers.0.weight', 'bbox_embed.2.layers.0.bias', 'bbox_embed.2.layers.1.weight', 'bbox_embed.2.layers.1.bias', 'bbox_embed.2.layers.2.weight', 'bbox_embed.2.layers.2.bias', 'bbox_embed.3.layers.0.weight', 'bbox_embed.3.layers.0.bias', 'bbox_embed.3.layers.1.weight', 'bbox_embed.3.layers.1.bias', 'bbox_embed.3.layers.2.weight', 'bbox_embed.3.layers.2.bias', 'bbox_embed.4.layers.0.weight', 'bbox_embed.4.layers.0.bias', 'bbox_embed.4.layers.1.weight', 'bbox_embed.4.layers.1.bias', 'bbox_embed.4.layers.2.weight', 'bbox_embed.4.layers.2.bias', 'bbox_embed.5.layers.0.weight', 'bbox_embed.5.layers.0.bias', 'bbox_embed.5.layers.1.weight', 'bbox_embed.5.layers.1.bias', 'bbox_embed.5.layers.2.weight', 'bbox_embed.5.layers.2.bias'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4400, training_loss=22.407134898792613, metrics={'train_runtime': 4330.7907, 'train_samples_per_second': 16.163, 'train_steps_per_second': 1.016, 'total_flos': 1.240948138752e+19, 'train_loss': 22.407134898792613, 'epoch': 100.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoModelForObjectDetection\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    anchor_image_size=None,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"rtdetr_model\",\n",
    "    num_train_epochs=100,\n",
    "    max_grad_norm=0.1,\n",
    "    learning_rate=1e-5,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    weight_decay=0.10,\n",
    "    gradient_accumulation_steps=2,\n",
    "    lr_scheduler_type=\"cosine\", \n",
    "    warmup_steps=300,\n",
    "    per_device_train_batch_size=8,\n",
    "    dataloader_num_workers=6,\n",
    "    metric_for_best_model=\"eval_map\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=4,\n",
    "    remove_unused_columns=False,\n",
    "    eval_do_concat_batches=False,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=image_processor,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=eval_compute_metrics_fn,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa2344-de30-4acb-915c-6fec2bfbbea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "from PIL import Image, ImageDraw\n",
    "from pprint import pprint\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=test_ds, metric_key_prefix=\"eval\")\n",
    "pprint(metrics)\n",
    "device = \"cuda\"\n",
    "\n",
    "image = Image.open(\"test.jpg\")\n",
    "\n",
    "inputs = image_processor(images=[image], return_tensors=\"pt\")\n",
    "inputs = inputs.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "target_sizes = torch.tensor([image.size[::-1]])\n",
    "\n",
    "result = image_processor.post_process_object_detection(outputs, threshold=0.3, target_sizes=target_sizes)[0]\n",
    "\n",
    "for score, label, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(\n",
    "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "        f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )\n",
    "\n",
    "image_with_boxes = image.copy()\n",
    "draw = ImageDraw.Draw(image_with_boxes)\n",
    "\n",
    "for score, label, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    x, y, x2, y2 = tuple(box)\n",
    "    draw.rectangle((x, y, x2, y2), outline=\"red\", width=1)\n",
    "    draw.text((x, y), model.config.id2label[label.item()], fill=\"white\")\n",
    "\n",
    "image_with_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b526762e-93a6-40ee-b188-b5d7bee8bcc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['processor/preprocessor_config.json']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"decent_model\")\n",
    "image_processor.save_pretrained(\"processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea1340c0-6a23-475f-9a2b-039df1791c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['animals', 'cat', 'chicken', 'cow', 'dog', 'fox', 'goat', 'horse', 'person', 'racoon', 'skunk']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not load the custom kernel for multi-scale deformable attention: Error building extension 'MultiScaleDeformableAttention': [1/2] /usr/local/cuda-11.5/bin/nvcc --generate-dependencies-with-compile --dependency-output ms_deform_attn_cuda.cuda.o.d -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/eagle/anaconda3/lib/python3.11/site-packages/transformers/kernels/deformable_detr -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include/TH -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda-11.5/include -isystem /home/eagle/anaconda3/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -std=c++17 -c /home/eagle/anaconda3/lib/python3.11/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu -o ms_deform_attn_cuda.cuda.o \n",
      "\u001b[31mFAILED: \u001b[0mms_deform_attn_cuda.cuda.o \n",
      "/usr/local/cuda-11.5/bin/nvcc --generate-dependencies-with-compile --dependency-output ms_deform_attn_cuda.cuda.o.d -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/eagle/anaconda3/lib/python3.11/site-packages/transformers/kernels/deformable_detr -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include/TH -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda-11.5/include -isystem /home/eagle/anaconda3/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -std=c++17 -c /home/eagle/anaconda3/lib/python3.11/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu -o ms_deform_attn_cuda.cuda.o \n",
      "/bin/sh: 1: /usr/local/cuda-11.5/bin/nvcc: not found\n",
      "ninja: build stopped: subcommand failed.\n",
      "\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/eagle/.cache/torch_extensions/py311_cu121/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/eagle/.cache/torch_extensions/py311_cu121/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/eagle/.cache/torch_extensions/py311_cu121/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/eagle/.cache/torch_extensions/py311_cu121/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/eagle/.cache/torch_extensions/py311_cu121/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x2a9d25c0) is not the object's thread (0x3306c9e0).\n",
      "Cannot move to target thread (0x2a9d25c0)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Run inference on the model\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 41\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Post-process the outputs to get boxes, labels, and scores\u001b[39;00m\n\u001b[1;32m     44\u001b[0m target_sizes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([rgb_frame\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]])\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Use the original image size for scaling\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/rt_detr/modeling_rt_detr.py:2616\u001b[0m, in \u001b[0;36mRTDetrForObjectDetection.forward\u001b[0;34m(self, pixel_values, pixel_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2610\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2611\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m   2612\u001b[0m )\n\u001b[1;32m   2614\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 2616\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   2617\u001b[0m     pixel_values,\n\u001b[1;32m   2618\u001b[0m     pixel_mask\u001b[38;5;241m=\u001b[39mpixel_mask,\n\u001b[1;32m   2619\u001b[0m     encoder_outputs\u001b[38;5;241m=\u001b[39mencoder_outputs,\n\u001b[1;32m   2620\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   2621\u001b[0m     decoder_inputs_embeds\u001b[38;5;241m=\u001b[39mdecoder_inputs_embeds,\n\u001b[1;32m   2622\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   2623\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   2624\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   2625\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   2626\u001b[0m )\n\u001b[1;32m   2628\u001b[0m denoising_meta_values \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2629\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mdenoising_meta_values \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2630\u001b[0m )\n\u001b[1;32m   2632\u001b[0m outputs_class \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mintermediate_logits \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/rt_detr/modeling_rt_detr.py:1751\u001b[0m, in \u001b[0;36mRTDetrModel.forward\u001b[0;34m(self, pixel_values, pixel_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1749\u001b[0m     pixel_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(((batch_size, height, width)), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m-> 1751\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(pixel_values, pixel_mask)\n\u001b[1;32m   1753\u001b[0m proj_feats \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_input_proj[level](source) \u001b[38;5;28;01mfor\u001b[39;00m level, (source, mask) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(features)]\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/rt_detr/modeling_rt_detr.py:570\u001b[0m, in \u001b[0;36mRTDetrConvEncoder.forward\u001b[0;34m(self, pixel_values, pixel_mask)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values: torch\u001b[38;5;241m.\u001b[39mTensor, pixel_mask: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;66;03m# send pixel_values through the model to get list of feature maps\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(pixel_values)\u001b[38;5;241m.\u001b[39mfeature_maps\n\u001b[1;32m    572\u001b[0m     out \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature_map \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;66;03m# downsample pixel_mask to match shape of corresponding feature_map\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/rt_detr/modeling_rt_detr_resnet.py:415\u001b[0m, in \u001b[0;36mRTDetrResNetBackbone.forward\u001b[0;34m(self, pixel_values, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    409\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    410\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    411\u001b[0m )\n\u001b[1;32m    413\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder(pixel_values)\n\u001b[0;32m--> 415\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(embedding_output, output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    417\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mhidden_states\n\u001b[1;32m    419\u001b[0m feature_maps \u001b[38;5;241m=\u001b[39m ()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/rt_detr/modeling_rt_detr_resnet.py:298\u001b[0m, in \u001b[0;36mRTDetrResNetEncoder.forward\u001b[0;34m(self, hidden_state, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    296\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m (hidden_state,)\n\u001b[0;32m--> 298\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m stage_module(hidden_state)\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    301\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m (hidden_state,)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/rt_detr/modeling_rt_detr_resnet.py:266\u001b[0m, in \u001b[0;36mRTDetrResNetStage.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    264\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 266\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m layer(hidden_state)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_state\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/rt_detr/modeling_rt_detr_resnet.py:226\u001b[0m, in \u001b[0;36mRTDetrResNetBottleNeckLayer.forward\u001b[0;34m(self, hidden_state)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_state):\n\u001b[1;32m    225\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_state\n\u001b[0;32m--> 226\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer(hidden_state)\n\u001b[1;32m    227\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshortcut(residual)\n\u001b[1;32m    228\u001b[0m     hidden_state \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m residual\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/rt_detr/modeling_rt_detr_resnet.py:64\u001b[0m, in \u001b[0;36mRTDetrResNetConvLayer.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 64\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvolution(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m     65\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalization(hidden_state)\n\u001b[1;32m     66\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(hidden_state)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from transformers import AutoModelForObjectDetection, AutoImageProcessor\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(categories)\n",
    "# Load the trained model and the image processor\n",
    "checkpoint_path = \"checkpoint-8750\"\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    anchor_image_size=None,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "processor = AutoImageProcessor.from_pretrained(checkpoint_path)\n",
    "model.to(device)\n",
    "# Open the camera (0 is the default camera, change if you have multiple cameras)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the camera resolution (optional)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 640)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()  # Capture frame-by-frame\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB (OpenCV uses BGR by default)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Preprocess the image\n",
    "    inputs = processor(images=rgb_frame, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Run inference on the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Post-process the outputs to get boxes, labels, and scores\n",
    "    target_sizes = torch.tensor([rgb_frame.shape[:2]]).to(device)  # Use the original image size for scaling\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes)[0]\n",
    "\n",
    "    # Extract the boxes, labels, and scores\n",
    "    boxes = results['boxes'].cpu().numpy()  # Bounding boxes\n",
    "    labels = results['labels'].cpu().numpy()  # Predicted class labels\n",
    "    scores = results['scores'].cpu().numpy()  # Confidence scores\n",
    "\n",
    "    # Draw the bounding boxes on the frame\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score > 0.3:  # Only show boxes with a confidence score > 0.5\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)  # Draw rectangle\n",
    "\n",
    "            # Display label and score\n",
    "            label_text = f\"{id2label[label]}: {score:.2f}\"\n",
    "            cv2.putText(frame, label_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "    # Display the frame with bounding boxes\n",
    "    cv2.imshow('Camera Feed - Object Detection', frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70eee655-0214-4640-85ff-d599823a6f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not load the custom kernel for multi-scale deformable attention: Error building extension 'MultiScaleDeformableAttention': [1/2] /usr/local/cuda-11.5/bin/nvcc --generate-dependencies-with-compile --dependency-output ms_deform_attn_cuda.cuda.o.d -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/eagle/anaconda3/lib/python3.11/site-packages/transformers/kernels/deformable_detr -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include/TH -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda-11.5/include -isystem /home/eagle/anaconda3/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -std=c++17 -c /home/eagle/anaconda3/lib/python3.11/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu -o ms_deform_attn_cuda.cuda.o \n",
      "\u001b[31mFAILED: \u001b[0mms_deform_attn_cuda.cuda.o \n",
      "/usr/local/cuda-11.5/bin/nvcc --generate-dependencies-with-compile --dependency-output ms_deform_attn_cuda.cuda.o.d -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/eagle/anaconda3/lib/python3.11/site-packages/transformers/kernels/deformable_detr -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include/TH -isystem /home/eagle/anaconda3/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda-11.5/include -isystem /home/eagle/anaconda3/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -std=c++17 -c /home/eagle/anaconda3/lib/python3.11/site-packages/transformers/kernels/deformable_detr/cuda/ms_deform_attn_cuda.cu -o ms_deform_attn_cuda.cuda.o \n",
      "/bin/sh: 1: /usr/local/cuda-11.5/bin/nvcc: not found\n",
      "ninja: build stopped: subcommand failed.\n",
      "\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/eagle/.cache/torch_extensions/py311_cu121/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/eagle/.cache/torch_extensions/py311_cu121/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/eagle/.cache/torch_extensions/py311_cu121/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/eagle/.cache/torch_extensions/py311_cu121/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/eagle/.cache/torch_extensions/py311_cu121/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n",
      "QObject::moveToThread: Current thread (0x31fde800) is not the object's thread (0x34a4be40).\n",
      "Cannot move to target thread (0x31fde800)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForObjectDetection, AutoImageProcessor\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the trained model and the image processor\n",
    "checkpoint_path = \"checkpoint-8750\"\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    anchor_image_size=None,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "processor = AutoImageProcessor.from_pretrained(checkpoint_path)\n",
    "model.to(device)\n",
    "\n",
    "# Open the camera (0 is the default camera, change if you have multiple cameras)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the camera resolution (optional)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 640)\n",
    "\n",
    "while True:\n",
    "    start_time = time.time()  # Start time for FPS calculation\n",
    "    \n",
    "    ret, frame = cap.read()  # Capture frame-by-frame\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB (OpenCV uses BGR by default)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Preprocess the image\n",
    "    inputs = processor(images=rgb_frame, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Run inference on the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Post-process the outputs to get boxes, labels, and scores\n",
    "    target_sizes = torch.tensor([rgb_frame.shape[:2]]).to(device)  # Use the original image size for scaling\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes)[0]\n",
    "\n",
    "    # Extract the boxes, labels, and scores\n",
    "    boxes = results['boxes'].cpu().numpy()  # Bounding boxes\n",
    "    labels = results['labels'].cpu().numpy()  # Predicted class labels\n",
    "    scores = results['scores'].cpu().numpy()  # Confidence scores\n",
    "\n",
    "    # Draw the bounding boxes on the frame\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score > 0.3:  # Only show boxes with a confidence score > 0.3\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)  # Draw rectangle\n",
    "\n",
    "            # Display label and score\n",
    "            label_text = f\"{id2label[label]}: {score:.2f}\"\n",
    "            cv2.putText(frame, label_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "    # Calculate FPS\n",
    "    end_time = time.time()\n",
    "    fps = 1 / (end_time - start_time)\n",
    "    cv2.putText(frame, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame with bounding boxes\n",
    "    cv2.imshow('Camera Feed - Object Detection', frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fecc21bc-020c-4eaa-9ab2-c86e1322eb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RTDetrForObjectDetection(\n",
      "  (model): RTDetrModel(\n",
      "    (backbone): RTDetrConvEncoder(\n",
      "      (model): RTDetrResNetBackbone(\n",
      "        (embedder): RTDetrResNetEmbeddings(\n",
      "          (embedder): Sequential(\n",
      "            (0): RTDetrResNetConvLayer(\n",
      "              (convolution): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (normalization): RTDetrFrozenBatchNorm2d()\n",
      "              (activation): ReLU()\n",
      "            )\n",
      "            (1): RTDetrResNetConvLayer(\n",
      "              (convolution): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (normalization): RTDetrFrozenBatchNorm2d()\n",
      "              (activation): ReLU()\n",
      "            )\n",
      "            (2): RTDetrResNetConvLayer(\n",
      "              (convolution): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (normalization): RTDetrFrozenBatchNorm2d()\n",
      "              (activation): ReLU()\n",
      "            )\n",
      "          )\n",
      "          (pooler): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "        )\n",
      "        (encoder): RTDetrResNetEncoder(\n",
      "          (stages): ModuleList(\n",
      "            (0): RTDetrResNetStage(\n",
      "              (layers): Sequential(\n",
      "                (0): RTDetrResNetBottleNeckLayer(\n",
      "                  (shortcut): RTDetrResNetShortCut(\n",
      "                    (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                    (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                  )\n",
      "                  (layer): Sequential(\n",
      "                    (0): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (1): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (2): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): Identity()\n",
      "                    )\n",
      "                  )\n",
      "                  (activation): ReLU()\n",
      "                )\n",
      "                (1): RTDetrResNetBottleNeckLayer(\n",
      "                  (shortcut): Identity()\n",
      "                  (layer): Sequential(\n",
      "                    (0): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (1): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (2): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): Identity()\n",
      "                    )\n",
      "                  )\n",
      "                  (activation): ReLU()\n",
      "                )\n",
      "                (2): RTDetrResNetBottleNeckLayer(\n",
      "                  (shortcut): Identity()\n",
      "                  (layer): Sequential(\n",
      "                    (0): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (1): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (2): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): Identity()\n",
      "                    )\n",
      "                  )\n",
      "                  (activation): ReLU()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): RTDetrResNetStage(\n",
      "              (layers): Sequential(\n",
      "                (0): RTDetrResNetBottleNeckLayer(\n",
      "                  (shortcut): Sequential(\n",
      "                    (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "                    (1): RTDetrResNetShortCut(\n",
      "                      (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                    )\n",
      "                  )\n",
      "                  (layer): Sequential(\n",
      "                    (0): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (1): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (2): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): Identity()\n",
      "                    )\n",
      "                  )\n",
      "                  (activation): ReLU()\n",
      "                )\n",
      "                (1): RTDetrResNetBottleNeckLayer(\n",
      "                  (shortcut): Identity()\n",
      "                  (layer): Sequential(\n",
      "                    (0): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (1): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (2): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): Identity()\n",
      "                    )\n",
      "                  )\n",
      "                  (activation): ReLU()\n",
      "                )\n",
      "                (2): RTDetrResNetBottleNeckLayer(\n",
      "                  (shortcut): Identity()\n",
      "                  (layer): Sequential(\n",
      "                    (0): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (1): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (2): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): Identity()\n",
      "                    )\n",
      "                  )\n",
      "                  (activation): ReLU()\n",
      "                )\n",
      "                (3): RTDetrResNetBottleNeckLayer(\n",
      "                  (shortcut): Identity()\n",
      "                  (layer): Sequential(\n",
      "                    (0): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (1): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (2): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): Identity()\n",
      "                    )\n",
      "                  )\n",
      "                  (activation): ReLU()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (2): RTDetrResNetStage(\n",
      "              (layers): Sequential(\n",
      "                (0): RTDetrResNetBottleNeckLayer(\n",
      "                  (shortcut): Sequential(\n",
      "                    (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "                    (1): RTDetrResNetShortCut(\n",
      "                      (convolution): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                    )\n",
      "                  )\n",
      "                  (layer): Sequential(\n",
      "                    (0): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (1): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (2): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): Identity()\n",
      "                    )\n",
      "                  )\n",
      "                  (activation): ReLU()\n",
      "                )\n",
      "                (1): RTDetrResNetBottleNeckLayer(\n",
      "                  (shortcut): Identity()\n",
      "                  (layer): Sequential(\n",
      "                    (0): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (1): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (2): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): Identity()\n",
      "                    )\n",
      "                  )\n",
      "                  (activation): ReLU()\n",
      "                )\n",
      "                (2): RTDetrResNetBottleNeckLayer(\n",
      "                  (shortcut): Identity()\n",
      "                  (layer): Sequential(\n",
      "                    (0): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (1): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (2): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): Identity()\n",
      "                    )\n",
      "                  )\n",
      "                  (activation): ReLU()\n",
      "                )\n",
      "                (3): RTDetrResNetBottleNeckLayer(\n",
      "                  (shortcut): Identity()\n",
      "                  (layer): Sequential(\n",
      "                    (0): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (1): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (2): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): Identity()\n",
      "                    )\n",
      "                  )\n",
      "                  (activation): ReLU()\n",
      "                )\n",
      "                (4): RTDetrResNetBottleNeckLayer(\n",
      "                  (shortcut): Identity()\n",
      "                  (layer): Sequential(\n",
      "                    (0): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (1): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (2): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): Identity()\n",
      "                    )\n",
      "                  )\n",
      "                  (activation): ReLU()\n",
      "                )\n",
      "                (5): RTDetrResNetBottleNeckLayer(\n",
      "                  (shortcut): Identity()\n",
      "                  (layer): Sequential(\n",
      "                    (0): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (1): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (2): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): Identity()\n",
      "                    )\n",
      "                  )\n",
      "                  (activation): ReLU()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (3): RTDetrResNetStage(\n",
      "              (layers): Sequential(\n",
      "                (0): RTDetrResNetBottleNeckLayer(\n",
      "                  (shortcut): Sequential(\n",
      "                    (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "                    (1): RTDetrResNetShortCut(\n",
      "                      (convolution): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                    )\n",
      "                  )\n",
      "                  (layer): Sequential(\n",
      "                    (0): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (1): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (2): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): Identity()\n",
      "                    )\n",
      "                  )\n",
      "                  (activation): ReLU()\n",
      "                )\n",
      "                (1): RTDetrResNetBottleNeckLayer(\n",
      "                  (shortcut): Identity()\n",
      "                  (layer): Sequential(\n",
      "                    (0): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (1): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (2): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): Identity()\n",
      "                    )\n",
      "                  )\n",
      "                  (activation): ReLU()\n",
      "                )\n",
      "                (2): RTDetrResNetBottleNeckLayer(\n",
      "                  (shortcut): Identity()\n",
      "                  (layer): Sequential(\n",
      "                    (0): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (1): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): ReLU()\n",
      "                    )\n",
      "                    (2): RTDetrResNetConvLayer(\n",
      "                      (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
      "                      (activation): Identity()\n",
      "                    )\n",
      "                  )\n",
      "                  (activation): ReLU()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (encoder_input_proj): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (encoder): RTDetrHybridEncoder(\n",
      "      (encoder): ModuleList(\n",
      "        (0): RTDetrEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0): RTDetrEncoderLayer(\n",
      "              (self_attn): RTDetrMultiheadAttention(\n",
      "                (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (activation_fn): GELUActivation()\n",
      "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (lateral_convs): ModuleList(\n",
      "        (0-1): 2 x RTDetrConvNormLayer(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (fpn_blocks): ModuleList(\n",
      "        (0-1): 2 x RTDetrCSPRepLayer(\n",
      "          (conv1): RTDetrConvNormLayer(\n",
      "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activation): SiLU()\n",
      "          )\n",
      "          (conv2): RTDetrConvNormLayer(\n",
      "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activation): SiLU()\n",
      "          )\n",
      "          (bottlenecks): Sequential(\n",
      "            (0): RTDetrRepVggBlock(\n",
      "              (conv1): RTDetrConvNormLayer(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (activation): Identity()\n",
      "              )\n",
      "              (conv2): RTDetrConvNormLayer(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (activation): Identity()\n",
      "              )\n",
      "              (activation): SiLU()\n",
      "            )\n",
      "            (1): RTDetrRepVggBlock(\n",
      "              (conv1): RTDetrConvNormLayer(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (activation): Identity()\n",
      "              )\n",
      "              (conv2): RTDetrConvNormLayer(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (activation): Identity()\n",
      "              )\n",
      "              (activation): SiLU()\n",
      "            )\n",
      "            (2): RTDetrRepVggBlock(\n",
      "              (conv1): RTDetrConvNormLayer(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (activation): Identity()\n",
      "              )\n",
      "              (conv2): RTDetrConvNormLayer(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (activation): Identity()\n",
      "              )\n",
      "              (activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (conv3): Identity()\n",
      "        )\n",
      "      )\n",
      "      (downsample_convs): ModuleList(\n",
      "        (0-1): 2 x RTDetrConvNormLayer(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (pan_blocks): ModuleList(\n",
      "        (0-1): 2 x RTDetrCSPRepLayer(\n",
      "          (conv1): RTDetrConvNormLayer(\n",
      "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activation): SiLU()\n",
      "          )\n",
      "          (conv2): RTDetrConvNormLayer(\n",
      "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activation): SiLU()\n",
      "          )\n",
      "          (bottlenecks): Sequential(\n",
      "            (0): RTDetrRepVggBlock(\n",
      "              (conv1): RTDetrConvNormLayer(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (activation): Identity()\n",
      "              )\n",
      "              (conv2): RTDetrConvNormLayer(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (activation): Identity()\n",
      "              )\n",
      "              (activation): SiLU()\n",
      "            )\n",
      "            (1): RTDetrRepVggBlock(\n",
      "              (conv1): RTDetrConvNormLayer(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (activation): Identity()\n",
      "              )\n",
      "              (conv2): RTDetrConvNormLayer(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (activation): Identity()\n",
      "              )\n",
      "              (activation): SiLU()\n",
      "            )\n",
      "            (2): RTDetrRepVggBlock(\n",
      "              (conv1): RTDetrConvNormLayer(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (activation): Identity()\n",
      "              )\n",
      "              (conv2): RTDetrConvNormLayer(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "                (activation): Identity()\n",
      "              )\n",
      "              (activation): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (conv3): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (denoising_class_embed): Embedding(12, 256, padding_idx=11)\n",
      "    (enc_output): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (enc_score_head): Linear(in_features=256, out_features=11, bias=True)\n",
      "    (enc_bbox_head): RTDetrMLPPredictionHead(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (decoder_input_proj): ModuleList(\n",
      "      (0-2): 3 x Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (decoder): RTDetrDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x RTDetrDecoderLayer(\n",
      "          (self_attn): RTDetrMultiheadAttention(\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): RTDetrMultiscaleDeformableAttention(\n",
      "            (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
      "            (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
      "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (query_pos_head): RTDetrMLPPredictionHead(\n",
      "        (layers): ModuleList(\n",
      "          (0): Linear(in_features=4, out_features=512, bias=True)\n",
      "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (class_embed): ModuleList(\n",
      "        (0-5): 6 x Linear(in_features=256, out_features=11, bias=True)\n",
      "      )\n",
      "      (bbox_embed): ModuleList(\n",
      "        (0-5): 6 x RTDetrMLPPredictionHead(\n",
      "          (layers): ModuleList(\n",
      "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (class_embed): ModuleList(\n",
      "    (0-5): 6 x Linear(in_features=256, out_features=11, bias=True)\n",
      "  )\n",
      "  (bbox_embed): ModuleList(\n",
      "    (0-5): 6 x RTDetrMLPPredictionHead(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d133cee0-d1a3-412c-a2c3-d120287b5a31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
