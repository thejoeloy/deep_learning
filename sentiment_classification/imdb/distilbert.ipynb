{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb9c701-0426-471a-bf1d-3d0b032eb1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eagle/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from transformers import DistilBertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "ds.pop('unsupervised')\n",
    "\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"test\"]\n",
    "\n",
    "# Split the train dataset into train and validation sets\n",
    "train_size = int(0.8 * len(train_ds))\n",
    "val_size = len(train_ds) - train_size\n",
    "train_ds, val_ds = random_split(train_ds, [train_size, val_size])\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256  # Adjust max_length for LSTM input size (e.g., 128, 256, etc., based on GPU memory)\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "train_ds = train_ds.dataset.map(tokenize_function, batched=True)\n",
    "val_ds = val_ds.dataset.map(tokenize_function, batched=True)\n",
    "test_ds = test_ds.map(tokenize_function, batched=True)\n",
    "\n",
    "# Keep only input_ids and attention_mask in the dataset for LSTM\n",
    "def format_for_lstm(batch):\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(batch[\"input_ids\"]),\n",
    "        \"attention_mask\": torch.tensor(batch[\"attention_mask\"]),\n",
    "        \"label\": torch.tensor(batch[\"label\"])\n",
    "    }\n",
    "\n",
    "train_ds = train_ds.map(format_for_lstm, batched=True)\n",
    "val_ds = val_ds.map(format_for_lstm, batched=True)\n",
    "test_ds = test_ds.map(format_for_lstm, batched=True)\n",
    "\n",
    "# Set the format for PyTorch compatibility\n",
    "train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=6)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=6)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed0b6de4-efc4-4fad-ace7-f748e0dcea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len  # Fixed typo in attribute name\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Create matrix of size seq_len x d_model\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape seq_len\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # Apply sin and cos\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        return self.dropout(x)  # Added return statement\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps: float = 10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        self.beta = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.beta  # Fixed bias to beta\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Added return statement\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
    "\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        assert d_model % h == 0, \"d_model is not appropriate size\"\n",
    "\n",
    "        self.d_k = d_model // h\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            # Reshape mask to match attention_scores shape [batch_size, num_heads, seq_len, seq_len]\n",
    "            if mask.dim() == 2:\n",
    "                mask = mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]\n",
    "                mask = mask.expand(-1, query.size(1), query.size(2), -1)  # [batch_size, num_heads, seq_len, seq_len]\n",
    "            \n",
    "            # Use a smaller negative value that works with float16\n",
    "            attention_scores.masked_fill_(mask == 0, -65504.0)  # Maximum negative value in float16\n",
    "            \n",
    "        attention_scores = attention_scores.softmax(dim=-1)\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "            \n",
    "        return (attention_scores @ value), attention_scores\n",
    "        \n",
    "    def forward(self, q, k, v, mask):\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # Linear transformations\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        query = query.view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        x, self_attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        # Combine heads and apply output transformation\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "        return self.w_o(x)\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x\n",
    "\n",
    "class DistilBERT(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, n_layers: int, n_heads: int, d_ff: int, dropout: float, num_classes: int = 1):\n",
    "        super().__init__()\n",
    "        self.input_embeddings = InputEmbeddings(d_model, vocab_size)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, seq_len=512, dropout=dropout)\n",
    "\n",
    "        # Create encoder blocks\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            EncoderBlock(\n",
    "                MultiHeadAttentionBlock(d_model, n_heads, dropout),\n",
    "                FeedForwardBlock(d_model, d_ff, dropout),\n",
    "                dropout\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Add classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Step 1: Get input embeddings and add positional encoding\n",
    "        x = self.input_embeddings(input_ids)\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        # Step 2: Pass through each encoder block\n",
    "        for layer in self.encoder_blocks:\n",
    "            x = layer(x, attention_mask)\n",
    "\n",
    "        # Step 3: Pool the output (use [CLS] token or mean pooling)\n",
    "        # Here we use mean pooling over the sequence length\n",
    "        # First multiply by attention mask to zero out padding tokens\n",
    "        if attention_mask is not None:\n",
    "            mask_expanded = attention_mask.unsqueeze(-1).float()\n",
    "            x = x * mask_expanded\n",
    "            # Calculate mean over sequence length (dim 1), excluding padding\n",
    "            sequence_lengths = torch.sum(attention_mask, dim=1, keepdim=True)\n",
    "            pooled = torch.sum(x, dim=1) / sequence_lengths\n",
    "        else:\n",
    "            # If no mask, just take mean over sequence length\n",
    "            pooled = torch.mean(x, dim=1)\n",
    "\n",
    "        # Step 4: Pass through classification head\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits\n",
    "\n",
    "# Example of instantiating the model\n",
    "vocab_size = 30522  # Typical vocab size for BERT\n",
    "d_model = 768  # Dimensionality of the embeddings\n",
    "n_layers = 6  # Number of transformer blocks\n",
    "n_heads = 12  # Number of attention heads\n",
    "d_ff = 3072  # Dimensionality of feedforward layer\n",
    "dropout = 0.1  # Dropout rate\n",
    "\n",
    "model = DistilBERT(vocab_size, d_model, n_layers, n_heads, d_ff, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "539357c5-563a-4762-900f-7e4a1a44778c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_186487/1861230628.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "Epoch [1/10] - Training:   0%|                          | 0/782 [00:00<?, ?it/s]/tmp/ipykernel_186487/1861230628.py:31: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch [1/10] - Validation:   0%|                        | 0/782 [00:00<?, ?it/s]/tmp/ipykernel_186487/1861230628.py:65: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 0.4717, Validation Loss: 0.3604, Validation Accuracy: 83.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Training Loss: 0.3342, Validation Loss: 0.2682, Validation Accuracy: 88.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Training Loss: 0.2876, Validation Loss: 0.2134, Validation Accuracy: 91.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Training Loss: 0.2470, Validation Loss: 0.1797, Validation Accuracy: 93.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Training Loss: 0.2110, Validation Loss: 0.1289, Validation Accuracy: 95.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Training Loss: 0.1773, Validation Loss: 0.0996, Validation Accuracy: 96.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Training Loss: 0.1484, Validation Loss: 0.0985, Validation Accuracy: 96.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Training Loss: 0.1248, Validation Loss: 0.0623, Validation Accuracy: 97.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Training Loss: 0.1044, Validation Loss: 0.0382, Validation Accuracy: 98.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Training Loss: 0.0887, Validation Loss: 0.0396, Validation Accuracy: 98.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()  # Use for binary classification\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.1)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "num_epochs = 10\n",
    "max_grad_norm = 0.1  # Set your max_grad_norm value\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Wrap train_loader in tqdm to monitor training progress\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch [{epoch + 1}/{num_epochs}] - Training\", leave=False)\n",
    "    \n",
    "    for batch in train_loader_tqdm:\n",
    "        # Ensure we access inputs correctly\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].float().to(device)  # Make sure labels are float for BCEWithLogitsLoss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass with mixed precision\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs, attention_mask)\n",
    "            loss = loss_fn(outputs.squeeze(), labels)  # Outputs should be raw logits\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Clip gradients\n",
    "        scaler.unscale_(optimizer)  # Unscale before gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Clip gradients\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        train_loader_tqdm.set_postfix(loss=loss.item())  # Display the current batch loss\n",
    "        \n",
    "    # Calculate average training loss for the epoch\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch [{epoch + 1}/{num_epochs}] - Validation\", leave=False)\n",
    "        \n",
    "        for batch in val_loader_tqdm:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].float().to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs, attention_mask)\n",
    "                loss = loss_fn(outputs.squeeze(), labels)\n",
    "                \n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted = (torch.sigmoid(outputs.squeeze()) > 0.5).long()  # Apply sigmoid and threshold\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.long()).sum().item()\n",
    "        \n",
    "            val_loader_tqdm.set_postfix(loss=loss.item())  # Display the current validation batch loss\n",
    "    \n",
    "    # Calculate average validation loss and accuracy\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "    accuracy = correct / total * 100\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "          f\"Training Loss: {avg_loss:.4f}, \"\n",
    "          f\"Validation Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"Validation Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d92460a-dd15-4530-8d82-6a7bcc834d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|                                          | 0/782 [00:00<?, ?it/s]/tmp/ipykernel_186487/1964194690.py:16: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8627, Test Accuracy: 82.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "running_test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "    \n",
    "# Define the loss function for evaluation\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loader_tqdm = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
    "        \n",
    "    for batch in test_loader_tqdm:\n",
    "        inputs, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].float().to(device)\n",
    "\n",
    "        # Forward pass with mixed precision\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs, attention_mask)\n",
    "            loss = loss_fn(outputs.squeeze(), labels)\n",
    "                \n",
    "        running_test_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted = (outputs.squeeze() > 0).long()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.long()).sum().item()\n",
    "\n",
    "        test_loader_tqdm.set_postfix(loss=loss.item())  # Display the current test batch loss\n",
    "    \n",
    "# Calculate average test loss and accuracy\n",
    "avg_test_loss = running_test_loss / len(test_loader)\n",
    "accuracy = correct / total * 100\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3870fc-695b-4754-b8bb-bc780083664e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
